{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment 4 - Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVhCarMwG70",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXJ1XpSwdvR",
        "colab_type": "text"
      },
      "source": [
        "Install and import all the necessary libraries for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMrFD-ZBwK",
        "colab_type": "code",
        "outputId": "59f73000-a12d-4dbb-9aa6-ba6d36c44214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 74kB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.27.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (45.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGgAUOKwsWA",
        "colab_type": "text"
      },
      "source": [
        "### **Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe2azQOdmND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gD5esSxfxjs",
        "colab_type": "code",
        "outputId": "b0fcf063-9bbb-4d46-ca57-8b3a1a8a1a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.25, random_state=7)\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=7)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "284 train examples\n",
            "95 validation examples\n",
            "127 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZTeC55HxDeT",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4GRPPLdTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZy7p3AaTRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4f7eeec9-4872-431c-82b8-1ce9d4f8b96b"
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KuTr4sxMl6",
        "colab_type": "text"
      },
      "source": [
        "### Defining Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380jHjPokFUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "\n",
        "\n",
        "CRIM=tf.feature_column.numeric_column(\"CRIM\")\n",
        "ZN=tf.feature_column.numeric_column(\"ZN\")\n",
        "INDUS=tf.feature_column.numeric_column(\"INDUS\")\n",
        "CHAS=tf.feature_column.numeric_column(\"CHAS\")\n",
        "NOX=tf.feature_column.numeric_column(\"NOX\")\n",
        "RM=tf.feature_column.numeric_column(\"RM\")\n",
        "AGE=tf.feature_column.numeric_column(\"AGE\")\n",
        "DIS=tf.feature_column.numeric_column(\"DIS\")\n",
        "RAD=tf.feature_column.numeric_column(\"RAD\")\n",
        "TAX=tf.feature_column.numeric_column(\"TAX\")\n",
        "PTRATIO=tf.feature_column.numeric_column(\"PTRATIO\")\n",
        "B=tf.feature_column.numeric_column(\"B\")\n",
        "LSTAT=tf.feature_column.numeric_column(\"LSTAT\")\n",
        "feature_columns = [CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb-wnXIlIIiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKaPehA6C3T8",
        "colab_type": "code",
        "outputId": "6fad6cd9-8132-4270-b738-1e264e5bf2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  target\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98    24.0\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14    21.6\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03    34.7\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94    33.4\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33    36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVCMrdMxVB5",
        "colab_type": "text"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAc9LpVzqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6B9FgRyyGXe",
        "colab_type": "text"
      },
      "source": [
        "Model should contain following layers:\n",
        "\n",
        "```\n",
        "feature_layer\n",
        "\n",
        "Dense(1, activation=None)\n",
        "```\n",
        "\n",
        "Use 'Adam' optimizer\n",
        "\n",
        "Use 'mse' as your loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInuZ8D0xsu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile your model in this cell."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn0ph-u9QBka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  \n",
        "  tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdzl3wasRo6",
        "colab_type": "code",
        "outputId": "a3f2ac86-a7b5-4e6b-eb62-9229c855de8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=600)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/600\n",
            "9/9 [==============================] - 1s 121ms/step - loss: 76030.1341 - mse: 76173.1641 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 74642.7566 - mse: 71914.3516 - val_loss: 58984.8698 - val_mse: 59055.7578\n",
            "Epoch 3/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 67802.1562 - mse: 67730.1797 - val_loss: 55471.5833 - val_mse: 55539.1641\n",
            "Epoch 4/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 64704.9700 - mse: 63756.0977 - val_loss: 52118.0729 - val_mse: 52182.4414\n",
            "Epoch 5/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 59161.9263 - mse: 59967.5508 - val_loss: 48934.4805 - val_mse: 48995.7461\n",
            "Epoch 6/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 56965.0379 - mse: 56423.1445 - val_loss: 45878.8555 - val_mse: 45937.1055\n",
            "Epoch 7/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 53656.7694 - mse: 52958.7305 - val_loss: 43020.8125 - val_mse: 43076.1758\n",
            "Epoch 8/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 49101.2874 - mse: 49750.7539 - val_loss: 40306.8086 - val_mse: 40359.3906\n",
            "Epoch 9/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 49584.2399 - mse: 46743.9492 - val_loss: 37717.7839 - val_mse: 37767.6562\n",
            "Epoch 10/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 43792.1548 - mse: 43789.0312 - val_loss: 35334.0677 - val_mse: 35381.3945\n",
            "Epoch 11/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 41516.2349 - mse: 41079.4727 - val_loss: 33079.5378 - val_mse: 33124.4141\n",
            "Epoch 12/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 39314.5387 - mse: 38586.9883 - val_loss: 30918.7109 - val_mse: 30961.1875\n",
            "Epoch 13/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 35857.3956 - mse: 36096.1250 - val_loss: 28946.6172 - val_mse: 28986.8555\n",
            "Epoch 14/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 34531.2090 - mse: 33869.1875 - val_loss: 27069.9297 - val_mse: 27107.9902\n",
            "Epoch 15/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 31472.2440 - mse: 31738.4121 - val_loss: 25319.9974 - val_mse: 25355.9785\n",
            "Epoch 16/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 29772.5864 - mse: 29721.2676 - val_loss: 23697.9401 - val_mse: 23731.9473\n",
            "Epoch 17/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 28138.0324 - mse: 27885.7500 - val_loss: 22164.1204 - val_mse: 22196.2148\n",
            "Epoch 18/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 25971.2515 - mse: 26117.0332 - val_loss: 20752.9883 - val_mse: 20783.2793\n",
            "Epoch 19/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 25352.6926 - mse: 24505.3203 - val_loss: 19429.1361 - val_mse: 19457.6875\n",
            "Epoch 20/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 23351.1624 - mse: 22949.5859 - val_loss: 18226.4124 - val_mse: 18253.3418\n",
            "Epoch 21/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 22422.6253 - mse: 21560.0371 - val_loss: 17095.5999 - val_mse: 17120.9609\n",
            "Epoch 22/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20273.4607 - mse: 20251.4980 - val_loss: 16047.6686 - val_mse: 16071.5342\n",
            "Epoch 23/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19124.0823 - mse: 19030.5801 - val_loss: 15078.1097 - val_mse: 15100.5518\n",
            "Epoch 24/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18425.9457 - mse: 17904.3848 - val_loss: 14180.0430 - val_mse: 14201.1250\n",
            "Epoch 25/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17468.4818 - mse: 16835.3848 - val_loss: 13367.1338 - val_mse: 13386.9443\n",
            "Epoch 26/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 15902.2984 - mse: 15860.1729 - val_loss: 12624.0234 - val_mse: 12642.6357\n",
            "Epoch 27/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 14969.3518 - mse: 14993.9258 - val_loss: 11921.6536 - val_mse: 11939.0957\n",
            "Epoch 28/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 13750.8524 - mse: 14136.9639 - val_loss: 11296.2135 - val_mse: 11312.5791\n",
            "Epoch 29/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 13890.0366 - mse: 13406.4385 - val_loss: 10703.0703 - val_mse: 10718.3828\n",
            "Epoch 30/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 12141.5684 - mse: 12672.9229 - val_loss: 10181.2660 - val_mse: 10195.6182\n",
            "Epoch 31/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 12431.9564 - mse: 12054.5137 - val_loss: 9686.3395 - val_mse: 9699.7500\n",
            "Epoch 32/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 10823.3565 - mse: 11416.6631 - val_loss: 9265.4440 - val_mse: 9278.0234\n",
            "Epoch 33/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 10675.6397 - mse: 10917.4980 - val_loss: 8850.6839 - val_mse: 8862.4141\n",
            "Epoch 34/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 10136.7159 - mse: 10387.8135 - val_loss: 8491.6276 - val_mse: 8502.5996\n",
            "Epoch 35/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 10197.6737 - mse: 9953.3281 - val_loss: 8149.9292 - val_mse: 8160.1553\n",
            "Epoch 36/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 9448.5652 - mse: 9505.2510 - val_loss: 7858.6297 - val_mse: 7868.1968\n",
            "Epoch 37/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 9000.5240 - mse: 9159.2627 - val_loss: 7570.7832 - val_mse: 7579.6763\n",
            "Epoch 38/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8813.4454 - mse: 8790.1826 - val_loss: 7319.8826 - val_mse: 7328.1689\n",
            "Epoch 39/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 8292.7324 - mse: 8455.1689 - val_loss: 7101.0682 - val_mse: 7108.8062\n",
            "Epoch 40/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 8283.0562 - mse: 8189.0308 - val_loss: 6886.1447 - val_mse: 6893.3291\n",
            "Epoch 41/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 7737.2976 - mse: 7896.9321 - val_loss: 6703.9896 - val_mse: 6710.6890\n",
            "Epoch 42/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 7469.3976 - mse: 7656.1973 - val_loss: 6534.4214 - val_mse: 6540.6597\n",
            "Epoch 43/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 7551.0372 - mse: 7428.6909 - val_loss: 6379.6989 - val_mse: 6385.5073\n",
            "Epoch 44/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6835.9915 - mse: 7217.0732 - val_loss: 6240.2619 - val_mse: 6245.6738\n",
            "Epoch 45/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6991.3858 - mse: 7039.4946 - val_loss: 6104.8431 - val_mse: 6109.8599\n",
            "Epoch 46/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 6929.6300 - mse: 6851.6118 - val_loss: 5986.3431 - val_mse: 5991.0176\n",
            "Epoch 47/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 6884.0011 - mse: 6695.1494 - val_loss: 5874.4661 - val_mse: 5878.8120\n",
            "Epoch 48/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6883.4563 - mse: 6542.5098 - val_loss: 5772.6003 - val_mse: 5776.6426\n",
            "Epoch 49/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 6235.4027 - mse: 6391.9546 - val_loss: 5684.4012 - val_mse: 5688.1841\n",
            "Epoch 50/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 5943.4325 - mse: 6260.7559 - val_loss: 5601.8929 - val_mse: 5605.4448\n",
            "Epoch 51/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6100.0135 - mse: 6152.2725 - val_loss: 5516.2423 - val_mse: 5519.5420\n",
            "Epoch 52/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6336.5990 - mse: 6047.0635 - val_loss: 5432.9982 - val_mse: 5436.0430\n",
            "Epoch 53/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5704.5000 - mse: 5919.7080 - val_loss: 5363.4583 - val_mse: 5366.3252\n",
            "Epoch 54/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 6090.0792 - mse: 5828.8950 - val_loss: 5291.9974 - val_mse: 5294.6738\n",
            "Epoch 55/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5885.4696 - mse: 5731.6660 - val_loss: 5225.2795 - val_mse: 5227.7817\n",
            "Epoch 56/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5446.8377 - mse: 5636.4443 - val_loss: 5163.1407 - val_mse: 5165.4995\n",
            "Epoch 57/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5214.2954 - mse: 5551.7676 - val_loss: 5102.6131 - val_mse: 5104.8271\n",
            "Epoch 58/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5532.9156 - mse: 5473.1025 - val_loss: 5041.0259 - val_mse: 5043.1040\n",
            "Epoch 59/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5055.1318 - mse: 5392.4106 - val_loss: 4982.8288 - val_mse: 4984.7803\n",
            "Epoch 60/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5383.3631 - mse: 5318.1636 - val_loss: 4924.0182 - val_mse: 4925.8574\n",
            "Epoch 61/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4676.7855 - mse: 5236.6304 - val_loss: 4870.9969 - val_mse: 4872.7520\n",
            "Epoch 62/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 5253.8558 - mse: 5168.7783 - val_loss: 4816.1060 - val_mse: 4817.7739\n",
            "Epoch 63/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4959.4791 - mse: 5104.5918 - val_loss: 4760.3938 - val_mse: 4761.9692\n",
            "Epoch 64/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5103.2791 - mse: 5032.6914 - val_loss: 4706.8429 - val_mse: 4708.3501\n",
            "Epoch 65/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4867.6783 - mse: 4968.6963 - val_loss: 4654.3950 - val_mse: 4655.8281\n",
            "Epoch 66/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 4670.1500 - mse: 4901.1987 - val_loss: 4602.7552 - val_mse: 4604.1465\n",
            "Epoch 67/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4923.3472 - mse: 4843.5576 - val_loss: 4548.7215 - val_mse: 4550.0601\n",
            "Epoch 68/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 4945.4030 - mse: 4779.5962 - val_loss: 4497.5239 - val_mse: 4498.8105\n",
            "Epoch 69/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 4811.5799 - mse: 4718.1611 - val_loss: 4446.1775 - val_mse: 4447.4321\n",
            "Epoch 70/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 4680.6483 - mse: 4661.4141 - val_loss: 4393.8883 - val_mse: 4395.1079\n",
            "Epoch 71/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4893.5423 - mse: 4599.2036 - val_loss: 4343.9380 - val_mse: 4345.1333\n",
            "Epoch 72/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 4551.0861 - mse: 4542.7388 - val_loss: 4293.3604 - val_mse: 4294.5322\n",
            "Epoch 73/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4486.5110 - mse: 4485.1421 - val_loss: 4242.1177 - val_mse: 4243.2827\n",
            "Epoch 74/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4615.7162 - mse: 4429.9385 - val_loss: 4191.6359 - val_mse: 4192.7788\n",
            "Epoch 75/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4478.1703 - mse: 4374.4331 - val_loss: 4141.5526 - val_mse: 4142.6758\n",
            "Epoch 76/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4262.5737 - mse: 4315.8716 - val_loss: 4092.9177 - val_mse: 4094.0349\n",
            "Epoch 77/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4330.9986 - mse: 4263.0220 - val_loss: 4043.6596 - val_mse: 4044.7651\n",
            "Epoch 78/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4313.9114 - mse: 4209.3691 - val_loss: 3993.6071 - val_mse: 3994.7073\n",
            "Epoch 79/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4254.5287 - mse: 4155.4883 - val_loss: 3943.3853 - val_mse: 3944.4861\n",
            "Epoch 80/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 4154.0293 - mse: 4100.6406 - val_loss: 3894.5138 - val_mse: 3895.6228\n",
            "Epoch 81/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4179.3947 - mse: 4048.4622 - val_loss: 3845.3569 - val_mse: 3846.4705\n",
            "Epoch 82/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 4019.1527 - mse: 3996.8186 - val_loss: 3796.0243 - val_mse: 3797.1428\n",
            "Epoch 83/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3845.9932 - mse: 3943.8694 - val_loss: 3748.4900 - val_mse: 3749.6067\n",
            "Epoch 84/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3953.6928 - mse: 3895.1064 - val_loss: 3699.5893 - val_mse: 3700.7024\n",
            "Epoch 85/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3878.7749 - mse: 3840.4275 - val_loss: 3652.4261 - val_mse: 3653.5500\n",
            "Epoch 86/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3593.1993 - mse: 3790.6980 - val_loss: 3604.9626 - val_mse: 3606.0940\n",
            "Epoch 87/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3848.7781 - mse: 3742.6008 - val_loss: 3557.4246 - val_mse: 3558.5527\n",
            "Epoch 88/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3489.0413 - mse: 3689.8948 - val_loss: 3509.7594 - val_mse: 3510.9109\n",
            "Epoch 89/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 3458.4560 - mse: 3640.3809 - val_loss: 3462.7963 - val_mse: 3463.9648\n",
            "Epoch 90/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3677.9068 - mse: 3593.9758 - val_loss: 3415.1134 - val_mse: 3416.2866\n",
            "Epoch 91/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3555.7084 - mse: 3542.2236 - val_loss: 3369.3205 - val_mse: 3370.5120\n",
            "Epoch 92/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3238.1983 - mse: 3494.4490 - val_loss: 3324.5294 - val_mse: 3325.7285\n",
            "Epoch 93/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3311.1884 - mse: 3446.9973 - val_loss: 3279.1286 - val_mse: 3280.3401\n",
            "Epoch 94/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3252.6350 - mse: 3400.5923 - val_loss: 3232.8355 - val_mse: 3234.0671\n",
            "Epoch 95/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3208.2259 - mse: 3354.7109 - val_loss: 3188.4959 - val_mse: 3189.7219\n",
            "Epoch 96/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3236.7527 - mse: 3306.2612 - val_loss: 3143.4739 - val_mse: 3144.7197\n",
            "Epoch 97/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3244.2992 - mse: 3261.1130 - val_loss: 3098.4547 - val_mse: 3099.7166\n",
            "Epoch 98/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3230.3599 - mse: 3215.3813 - val_loss: 3054.6659 - val_mse: 3055.9368\n",
            "Epoch 99/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 3092.8219 - mse: 3170.0935 - val_loss: 3012.2664 - val_mse: 3013.5420\n",
            "Epoch 100/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3035.5099 - mse: 3125.1240 - val_loss: 2969.6889 - val_mse: 2970.9763\n",
            "Epoch 101/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3200.7892 - mse: 3081.3369 - val_loss: 2927.4540 - val_mse: 2928.7517\n",
            "Epoch 102/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3026.3736 - mse: 3037.4846 - val_loss: 2885.0072 - val_mse: 2886.3247\n",
            "Epoch 103/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2986.5770 - mse: 2994.2942 - val_loss: 2842.7324 - val_mse: 2844.0745\n",
            "Epoch 104/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2977.4575 - mse: 2950.7170 - val_loss: 2802.2379 - val_mse: 2803.5967\n",
            "Epoch 105/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2838.1618 - mse: 2909.4285 - val_loss: 2762.5888 - val_mse: 2763.9495\n",
            "Epoch 106/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2821.2709 - mse: 2867.9705 - val_loss: 2722.1548 - val_mse: 2723.5266\n",
            "Epoch 107/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2668.0809 - mse: 2826.5408 - val_loss: 2682.1067 - val_mse: 2683.4888\n",
            "Epoch 108/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2597.2736 - mse: 2784.7988 - val_loss: 2642.6831 - val_mse: 2644.0767\n",
            "Epoch 109/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2762.6093 - mse: 2743.8242 - val_loss: 2603.7312 - val_mse: 2605.1396\n",
            "Epoch 110/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2678.3167 - mse: 2705.1448 - val_loss: 2565.2250 - val_mse: 2566.6370\n",
            "Epoch 111/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2729.3135 - mse: 2664.5903 - val_loss: 2526.7720 - val_mse: 2528.2004\n",
            "Epoch 112/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2549.2169 - mse: 2626.8887 - val_loss: 2488.8444 - val_mse: 2490.2756\n",
            "Epoch 113/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2524.1483 - mse: 2585.8601 - val_loss: 2451.9665 - val_mse: 2453.4133\n",
            "Epoch 114/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2518.6061 - mse: 2548.4319 - val_loss: 2414.5339 - val_mse: 2416.0005\n",
            "Epoch 115/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2449.3833 - mse: 2510.6934 - val_loss: 2377.7029 - val_mse: 2379.1885\n",
            "Epoch 116/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2590.0542 - mse: 2474.9875 - val_loss: 2341.6199 - val_mse: 2343.1001\n",
            "Epoch 117/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2526.7613 - mse: 2436.5181 - val_loss: 2306.9291 - val_mse: 2308.4109\n",
            "Epoch 118/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2383.8555 - mse: 2399.9104 - val_loss: 2271.9933 - val_mse: 2273.4863\n",
            "Epoch 119/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2303.8201 - mse: 2364.0835 - val_loss: 2237.2947 - val_mse: 2238.7991\n",
            "Epoch 120/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2369.2544 - mse: 2328.5640 - val_loss: 2202.8745 - val_mse: 2204.3928\n",
            "Epoch 121/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2258.0050 - mse: 2293.6394 - val_loss: 2169.2055 - val_mse: 2170.7317\n",
            "Epoch 122/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2264.0854 - mse: 2259.4521 - val_loss: 2135.4859 - val_mse: 2137.0178\n",
            "Epoch 123/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2302.2762 - mse: 2224.9141 - val_loss: 2102.9170 - val_mse: 2104.4500\n",
            "Epoch 124/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2281.1673 - mse: 2192.2280 - val_loss: 2069.9893 - val_mse: 2071.5249\n",
            "Epoch 125/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2201.7015 - mse: 2157.3630 - val_loss: 2038.9804 - val_mse: 2040.5178\n",
            "Epoch 126/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2141.3253 - mse: 2125.1704 - val_loss: 2006.6401 - val_mse: 2008.1918\n",
            "Epoch 127/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2206.0196 - mse: 2092.7783 - val_loss: 1974.7585 - val_mse: 1976.3303\n",
            "Epoch 128/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2045.8921 - mse: 2059.4099 - val_loss: 1945.5382 - val_mse: 1947.1105\n",
            "Epoch 129/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 2050.0942 - mse: 2029.8770 - val_loss: 1914.8782 - val_mse: 1916.4515\n",
            "Epoch 130/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1902.2293 - mse: 1997.5280 - val_loss: 1884.6562 - val_mse: 1886.2428\n",
            "Epoch 131/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1898.8782 - mse: 1966.6747 - val_loss: 1855.4189 - val_mse: 1857.0071\n",
            "Epoch 132/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2045.0762 - mse: 1937.7684 - val_loss: 1826.4150 - val_mse: 1827.9938\n",
            "Epoch 133/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1900.2399 - mse: 1905.9830 - val_loss: 1797.5375 - val_mse: 1799.1282\n",
            "Epoch 134/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1879.8491 - mse: 1876.6222 - val_loss: 1769.1290 - val_mse: 1770.7278\n",
            "Epoch 135/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1930.3277 - mse: 1848.2683 - val_loss: 1741.5261 - val_mse: 1743.1183\n",
            "Epoch 136/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1839.7830 - mse: 1818.9752 - val_loss: 1713.7973 - val_mse: 1715.3953\n",
            "Epoch 137/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1740.0849 - mse: 1789.6997 - val_loss: 1686.9692 - val_mse: 1688.5778\n",
            "Epoch 138/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1775.5767 - mse: 1763.2950 - val_loss: 1659.5683 - val_mse: 1661.1786\n",
            "Epoch 139/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1647.5033 - mse: 1735.3192 - val_loss: 1633.9479 - val_mse: 1635.5502\n",
            "Epoch 140/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1631.2125 - mse: 1707.4230 - val_loss: 1607.5105 - val_mse: 1609.1234\n",
            "Epoch 141/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1690.2412 - mse: 1681.3987 - val_loss: 1581.2155 - val_mse: 1582.8292\n",
            "Epoch 142/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1691.9361 - mse: 1654.5173 - val_loss: 1555.3418 - val_mse: 1556.9611\n",
            "Epoch 143/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1689.2116 - mse: 1628.4829 - val_loss: 1530.1372 - val_mse: 1531.7587\n",
            "Epoch 144/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1621.4590 - mse: 1601.9869 - val_loss: 1506.3428 - val_mse: 1507.9620\n",
            "Epoch 145/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1585.2078 - mse: 1576.8838 - val_loss: 1482.6327 - val_mse: 1484.2516\n",
            "Epoch 146/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1570.2826 - mse: 1552.3944 - val_loss: 1458.5256 - val_mse: 1460.1497\n",
            "Epoch 147/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1379.4998 - mse: 1528.0115 - val_loss: 1435.1328 - val_mse: 1436.7755\n",
            "Epoch 148/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1487.3597 - mse: 1504.5015 - val_loss: 1411.4621 - val_mse: 1413.0942\n",
            "Epoch 149/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1565.3760 - mse: 1480.6934 - val_loss: 1388.2072 - val_mse: 1389.8292\n",
            "Epoch 150/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1426.4866 - mse: 1455.6456 - val_loss: 1365.8463 - val_mse: 1367.4724\n",
            "Epoch 151/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1401.1074 - mse: 1433.3477 - val_loss: 1343.6818 - val_mse: 1345.3013\n",
            "Epoch 152/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1466.1650 - mse: 1409.9744 - val_loss: 1321.8388 - val_mse: 1323.4591\n",
            "Epoch 153/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1414.4688 - mse: 1387.5334 - val_loss: 1300.5071 - val_mse: 1302.1263\n",
            "Epoch 154/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1321.3406 - mse: 1365.3907 - val_loss: 1279.6278 - val_mse: 1281.2449\n",
            "Epoch 155/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1327.6846 - mse: 1343.9519 - val_loss: 1258.6637 - val_mse: 1260.2806\n",
            "Epoch 156/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1346.7361 - mse: 1322.6624 - val_loss: 1237.7170 - val_mse: 1239.3368\n",
            "Epoch 157/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1249.4076 - mse: 1300.7864 - val_loss: 1217.8719 - val_mse: 1219.4919\n",
            "Epoch 158/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1333.4032 - mse: 1281.1122 - val_loss: 1197.5395 - val_mse: 1199.1541\n",
            "Epoch 159/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1210.5155 - mse: 1259.5717 - val_loss: 1178.3694 - val_mse: 1179.9807\n",
            "Epoch 160/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1251.0745 - mse: 1240.4098 - val_loss: 1158.5548 - val_mse: 1160.1658\n",
            "Epoch 161/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1215.5872 - mse: 1219.9760 - val_loss: 1139.6043 - val_mse: 1141.2126\n",
            "Epoch 162/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1239.1197 - mse: 1201.0967 - val_loss: 1121.5440 - val_mse: 1123.1370\n",
            "Epoch 163/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1216.3708 - mse: 1181.5262 - val_loss: 1102.7182 - val_mse: 1104.3141\n",
            "Epoch 164/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1223.0912 - mse: 1162.3535 - val_loss: 1084.6283 - val_mse: 1086.2253\n",
            "Epoch 165/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1210.2858 - mse: 1143.7954 - val_loss: 1066.9661 - val_mse: 1068.5709\n",
            "Epoch 166/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1147.9213 - mse: 1126.6274 - val_loss: 1049.2126 - val_mse: 1050.8093\n",
            "Epoch 167/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1119.9173 - mse: 1107.5275 - val_loss: 1032.3072 - val_mse: 1033.9036\n",
            "Epoch 168/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1051.8627 - mse: 1090.4677 - val_loss: 1015.2247 - val_mse: 1016.8161\n",
            "Epoch 169/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1084.7180 - mse: 1072.7963 - val_loss: 998.4684 - val_mse: 1000.0568\n",
            "Epoch 170/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1054.7092 - mse: 1055.4219 - val_loss: 982.0688 - val_mse: 983.6565\n",
            "Epoch 171/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1053.9728 - mse: 1038.7056 - val_loss: 966.0710 - val_mse: 967.6492\n",
            "Epoch 172/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1032.9632 - mse: 1021.9593 - val_loss: 950.3367 - val_mse: 951.9082\n",
            "Epoch 173/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 981.2200 - mse: 1005.4330 - val_loss: 934.8068 - val_mse: 936.3799\n",
            "Epoch 174/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 977.8044 - mse: 990.1221 - val_loss: 919.2645 - val_mse: 920.8262\n",
            "Epoch 175/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 988.0073 - mse: 974.2162 - val_loss: 903.7936 - val_mse: 905.3503\n",
            "Epoch 176/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 942.6001 - mse: 957.8171 - val_loss: 889.2171 - val_mse: 890.7723\n",
            "Epoch 177/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 927.4449 - mse: 942.6057 - val_loss: 874.9813 - val_mse: 876.5294\n",
            "Epoch 178/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 931.4272 - mse: 928.1655 - val_loss: 860.3058 - val_mse: 861.8548\n",
            "Epoch 179/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 928.1205 - mse: 913.3169 - val_loss: 846.2686 - val_mse: 847.8057\n",
            "Epoch 180/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 843.9941 - mse: 898.3543 - val_loss: 832.4844 - val_mse: 834.0223\n",
            "Epoch 181/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 899.0103 - mse: 884.4227 - val_loss: 818.6600 - val_mse: 820.1929\n",
            "Epoch 182/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 882.3113 - mse: 870.5783 - val_loss: 804.9085 - val_mse: 806.4332\n",
            "Epoch 183/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 861.0189 - mse: 856.6898 - val_loss: 791.4046 - val_mse: 792.9233\n",
            "Epoch 184/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 829.5837 - mse: 842.7972 - val_loss: 778.4390 - val_mse: 779.9496\n",
            "Epoch 185/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 843.7570 - mse: 829.1478 - val_loss: 765.8025 - val_mse: 767.3076\n",
            "Epoch 186/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 817.1512 - mse: 816.3111 - val_loss: 753.1354 - val_mse: 754.6404\n",
            "Epoch 187/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 812.5915 - mse: 803.7563 - val_loss: 740.5037 - val_mse: 742.0002\n",
            "Epoch 188/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 779.8229 - mse: 790.8122 - val_loss: 728.3399 - val_mse: 729.8268\n",
            "Epoch 189/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 784.4342 - mse: 778.1376 - val_loss: 716.4615 - val_mse: 717.9431\n",
            "Epoch 190/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 779.4794 - mse: 766.2755 - val_loss: 704.4535 - val_mse: 705.9309\n",
            "Epoch 191/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 741.7631 - mse: 753.8235 - val_loss: 693.0040 - val_mse: 694.4746\n",
            "Epoch 192/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 703.2675 - mse: 741.8465 - val_loss: 681.8201 - val_mse: 683.2917\n",
            "Epoch 193/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 742.1178 - mse: 730.5710 - val_loss: 670.5648 - val_mse: 672.0248\n",
            "Epoch 194/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 741.0897 - mse: 719.3288 - val_loss: 659.2606 - val_mse: 660.7167\n",
            "Epoch 195/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 688.1209 - mse: 707.3712 - val_loss: 648.8206 - val_mse: 650.2711\n",
            "Epoch 196/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 718.7801 - mse: 697.6480 - val_loss: 637.5330 - val_mse: 638.9737\n",
            "Epoch 197/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 703.8985 - mse: 685.9517 - val_loss: 627.1493 - val_mse: 628.5826\n",
            "Epoch 198/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 696.7221 - mse: 675.0982 - val_loss: 617.1165 - val_mse: 618.5471\n",
            "Epoch 199/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 686.7001 - mse: 664.7386 - val_loss: 607.1758 - val_mse: 608.6003\n",
            "Epoch 200/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 656.9280 - mse: 654.7510 - val_loss: 597.2592 - val_mse: 598.6818\n",
            "Epoch 201/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 618.7023 - mse: 644.2808 - val_loss: 587.7987 - val_mse: 589.2157\n",
            "Epoch 202/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 658.5993 - mse: 634.8981 - val_loss: 578.0337 - val_mse: 579.4389\n",
            "Epoch 203/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 624.4846 - mse: 625.0311 - val_loss: 568.5776 - val_mse: 569.9739\n",
            "Epoch 204/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 646.7309 - mse: 615.4305 - val_loss: 559.2775 - val_mse: 560.6700\n",
            "Epoch 205/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 607.2931 - mse: 605.9558 - val_loss: 550.2040 - val_mse: 551.5886\n",
            "Epoch 206/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 630.3483 - mse: 596.7137 - val_loss: 541.3065 - val_mse: 542.6817\n",
            "Epoch 207/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 604.5615 - mse: 587.6695 - val_loss: 532.7917 - val_mse: 534.1654\n",
            "Epoch 208/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 577.6135 - mse: 578.9561 - val_loss: 524.0527 - val_mse: 525.4172\n",
            "Epoch 209/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 579.8419 - mse: 569.9849 - val_loss: 515.6703 - val_mse: 517.0253\n",
            "Epoch 210/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 567.0915 - mse: 561.7845 - val_loss: 507.1752 - val_mse: 508.5222\n",
            "Epoch 211/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 570.7576 - mse: 552.9830 - val_loss: 499.0600 - val_mse: 500.4030\n",
            "Epoch 212/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 541.5890 - mse: 544.7114 - val_loss: 491.0798 - val_mse: 492.4135\n",
            "Epoch 213/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 527.4564 - mse: 536.6278 - val_loss: 483.1725 - val_mse: 484.5001\n",
            "Epoch 214/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 527.0834 - mse: 528.6893 - val_loss: 475.4514 - val_mse: 476.7764\n",
            "Epoch 215/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 510.0827 - mse: 520.6497 - val_loss: 468.0476 - val_mse: 469.3667\n",
            "Epoch 216/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 518.6281 - mse: 513.1569 - val_loss: 460.4131 - val_mse: 461.7191\n",
            "Epoch 217/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 499.4449 - mse: 505.2876 - val_loss: 453.2006 - val_mse: 454.4990\n",
            "Epoch 218/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 502.1675 - mse: 498.0382 - val_loss: 445.9423 - val_mse: 447.2338\n",
            "Epoch 219/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 476.1520 - mse: 490.9596 - val_loss: 438.7063 - val_mse: 439.9877\n",
            "Epoch 220/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 475.7736 - mse: 483.5908 - val_loss: 431.7980 - val_mse: 433.0761\n",
            "Epoch 221/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 460.0052 - mse: 476.4778 - val_loss: 425.0145 - val_mse: 426.2842\n",
            "Epoch 222/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 467.4302 - mse: 469.6669 - val_loss: 418.2828 - val_mse: 419.5427\n",
            "Epoch 223/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 460.9273 - mse: 462.8177 - val_loss: 411.7727 - val_mse: 413.0294\n",
            "Epoch 224/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 475.9019 - mse: 456.4065 - val_loss: 405.1510 - val_mse: 406.3964\n",
            "Epoch 225/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 446.1839 - mse: 449.3419 - val_loss: 399.2509 - val_mse: 400.4955\n",
            "Epoch 226/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 429.1104 - mse: 443.2332 - val_loss: 393.0431 - val_mse: 394.2816\n",
            "Epoch 227/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 452.7072 - mse: 437.2069 - val_loss: 386.6847 - val_mse: 387.9134\n",
            "Epoch 228/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 427.9432 - mse: 430.8658 - val_loss: 380.5700 - val_mse: 381.7898\n",
            "Epoch 229/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 424.6605 - mse: 424.5643 - val_loss: 374.7864 - val_mse: 375.9957\n",
            "Epoch 230/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 416.5067 - mse: 418.8776 - val_loss: 368.9361 - val_mse: 370.1363\n",
            "Epoch 231/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 412.5350 - mse: 413.2061 - val_loss: 363.0956 - val_mse: 364.2887\n",
            "Epoch 232/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 420.8565 - mse: 407.4233 - val_loss: 357.6190 - val_mse: 358.8099\n",
            "Epoch 233/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 417.9027 - mse: 401.5786 - val_loss: 352.3377 - val_mse: 353.5251\n",
            "Epoch 234/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 401.3710 - mse: 396.0456 - val_loss: 347.0274 - val_mse: 348.2046\n",
            "Epoch 235/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 395.3971 - mse: 390.8545 - val_loss: 341.6684 - val_mse: 342.8349\n",
            "Epoch 236/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 382.1601 - mse: 385.3997 - val_loss: 336.6144 - val_mse: 337.7749\n",
            "Epoch 237/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 370.6258 - mse: 380.3126 - val_loss: 331.8746 - val_mse: 333.0342\n",
            "Epoch 238/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 379.4404 - mse: 375.4069 - val_loss: 326.6457 - val_mse: 327.7933\n",
            "Epoch 239/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 371.2554 - mse: 370.0666 - val_loss: 321.7840 - val_mse: 322.9231\n",
            "Epoch 240/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 377.9185 - mse: 365.1956 - val_loss: 316.9980 - val_mse: 318.1291\n",
            "Epoch 241/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 369.0430 - mse: 360.6243 - val_loss: 312.1103 - val_mse: 313.2315\n",
            "Epoch 242/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 355.3085 - mse: 355.5592 - val_loss: 307.6443 - val_mse: 308.7587\n",
            "Epoch 243/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 339.1848 - mse: 350.8963 - val_loss: 303.4323 - val_mse: 304.5440\n",
            "Epoch 244/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 346.1913 - mse: 346.6525 - val_loss: 298.7827 - val_mse: 299.8850\n",
            "Epoch 245/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 344.2991 - mse: 342.1009 - val_loss: 294.3009 - val_mse: 295.3947\n",
            "Epoch 246/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 343.2425 - mse: 337.5931 - val_loss: 289.9977 - val_mse: 291.0812\n",
            "Epoch 247/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 336.2897 - mse: 333.2627 - val_loss: 285.9435 - val_mse: 287.0224\n",
            "Epoch 248/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 331.0552 - mse: 329.0556 - val_loss: 281.8423 - val_mse: 282.9134\n",
            "Epoch 249/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 344.6831 - mse: 325.2479 - val_loss: 277.5885 - val_mse: 278.6483\n",
            "Epoch 250/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 324.5079 - mse: 320.7683 - val_loss: 273.8862 - val_mse: 274.9415\n",
            "Epoch 251/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 319.3357 - mse: 317.0481 - val_loss: 270.2528 - val_mse: 271.3060\n",
            "Epoch 252/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 323.8909 - mse: 313.1596 - val_loss: 266.3648 - val_mse: 267.4082\n",
            "Epoch 253/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 302.9372 - mse: 309.2459 - val_loss: 262.7487 - val_mse: 263.7863\n",
            "Epoch 254/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 315.8893 - mse: 305.7334 - val_loss: 258.8690 - val_mse: 259.8960\n",
            "Epoch 255/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 308.2208 - mse: 301.8555 - val_loss: 255.3276 - val_mse: 256.3482\n",
            "Epoch 256/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 300.2917 - mse: 298.0947 - val_loss: 252.1038 - val_mse: 253.1200\n",
            "Epoch 257/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 308.2831 - mse: 294.8464 - val_loss: 248.5503 - val_mse: 249.5579\n",
            "Epoch 258/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 286.7158 - mse: 291.2612 - val_loss: 245.2344 - val_mse: 246.2347\n",
            "Epoch 259/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 285.4388 - mse: 288.0950 - val_loss: 242.1334 - val_mse: 243.1305\n",
            "Epoch 260/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 264.0542 - mse: 284.5793 - val_loss: 238.7540 - val_mse: 239.7400\n",
            "Epoch 261/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 276.0665 - mse: 281.2991 - val_loss: 235.6156 - val_mse: 236.5946\n",
            "Epoch 262/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 282.6739 - mse: 278.1788 - val_loss: 232.3702 - val_mse: 233.3402\n",
            "Epoch 263/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 258.3946 - mse: 275.1391 - val_loss: 229.2015 - val_mse: 230.1624\n",
            "Epoch 264/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 275.4127 - mse: 271.8538 - val_loss: 226.4096 - val_mse: 227.3670\n",
            "Epoch 265/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 264.2878 - mse: 268.9345 - val_loss: 223.5407 - val_mse: 224.4922\n",
            "Epoch 266/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 267.3565 - mse: 265.9315 - val_loss: 220.7224 - val_mse: 221.6679\n",
            "Epoch 267/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 262.5273 - mse: 263.2549 - val_loss: 217.6813 - val_mse: 218.6162\n",
            "Epoch 268/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 258.5976 - mse: 260.2043 - val_loss: 215.1793 - val_mse: 216.1113\n",
            "Epoch 269/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 254.0284 - mse: 257.3738 - val_loss: 212.6241 - val_mse: 213.5504\n",
            "Epoch 270/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 259.2879 - mse: 254.7576 - val_loss: 209.9704 - val_mse: 210.8899\n",
            "Epoch 271/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 247.9958 - mse: 251.9683 - val_loss: 207.3617 - val_mse: 208.2728\n",
            "Epoch 272/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 250.6350 - mse: 249.7002 - val_loss: 204.5025 - val_mse: 205.4014\n",
            "Epoch 273/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 232.8303 - mse: 246.8723 - val_loss: 202.3205 - val_mse: 203.2181\n",
            "Epoch 274/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 249.8944 - mse: 244.3457 - val_loss: 199.8390 - val_mse: 200.7279\n",
            "Epoch 275/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 251.8527 - mse: 241.9551 - val_loss: 197.4566 - val_mse: 198.3403\n",
            "Epoch 276/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 242.7518 - mse: 239.4898 - val_loss: 195.1746 - val_mse: 196.0526\n",
            "Epoch 277/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 232.5891 - mse: 237.0191 - val_loss: 192.9059 - val_mse: 193.7764\n",
            "Epoch 278/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 224.4728 - mse: 234.8733 - val_loss: 190.5001 - val_mse: 191.3610\n",
            "Epoch 279/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 239.1665 - mse: 232.5184 - val_loss: 188.5064 - val_mse: 189.3641\n",
            "Epoch 280/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 215.3452 - mse: 230.2334 - val_loss: 186.5447 - val_mse: 187.3985\n",
            "Epoch 281/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 212.5383 - mse: 228.1212 - val_loss: 184.3137 - val_mse: 185.1583\n",
            "Epoch 282/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 229.5086 - mse: 225.9139 - val_loss: 182.3528 - val_mse: 183.1926\n",
            "Epoch 283/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 226.0320 - mse: 223.8698 - val_loss: 180.5817 - val_mse: 181.4189\n",
            "Epoch 284/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 227.7333 - mse: 221.8386 - val_loss: 178.4269 - val_mse: 179.2557\n",
            "Epoch 285/600\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 230.8804 - mse: 219.7674 - val_loss: 176.4739 - val_mse: 177.2958\n",
            "Epoch 286/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 207.1857 - mse: 217.7786 - val_loss: 174.6807 - val_mse: 175.4976\n",
            "Epoch 287/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 218.5065 - mse: 215.8152 - val_loss: 172.7368 - val_mse: 173.5452\n",
            "Epoch 288/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 225.0170 - mse: 213.9737 - val_loss: 170.8834 - val_mse: 171.6849\n",
            "Epoch 289/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 212.3092 - mse: 212.2230 - val_loss: 169.1197 - val_mse: 169.9154\n",
            "Epoch 290/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 207.7264 - mse: 210.3475 - val_loss: 167.2846 - val_mse: 168.0729\n",
            "Epoch 291/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 196.5636 - mse: 208.5192 - val_loss: 165.5813 - val_mse: 166.3632\n",
            "Epoch 292/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 216.4425 - mse: 206.7422 - val_loss: 164.0890 - val_mse: 164.8671\n",
            "Epoch 293/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 192.6171 - mse: 204.9420 - val_loss: 162.6024 - val_mse: 163.3760\n",
            "Epoch 294/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 204.2373 - mse: 203.4302 - val_loss: 161.1276 - val_mse: 161.8974\n",
            "Epoch 295/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 192.9417 - mse: 201.7192 - val_loss: 159.3786 - val_mse: 160.1389\n",
            "Epoch 296/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 195.7564 - mse: 200.1636 - val_loss: 157.7508 - val_mse: 158.5044\n",
            "Epoch 297/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 207.4480 - mse: 198.5004 - val_loss: 156.2576 - val_mse: 157.0047\n",
            "Epoch 298/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 193.4657 - mse: 196.9039 - val_loss: 154.9622 - val_mse: 155.7055\n",
            "Epoch 299/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 199.1622 - mse: 195.5437 - val_loss: 153.7923 - val_mse: 154.5345\n",
            "Epoch 300/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 178.3625 - mse: 193.8848 - val_loss: 152.4140 - val_mse: 153.1504\n",
            "Epoch 301/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 198.9656 - mse: 192.4592 - val_loss: 150.9586 - val_mse: 151.6887\n",
            "Epoch 302/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 195.3995 - mse: 191.1113 - val_loss: 149.4440 - val_mse: 150.1663\n",
            "Epoch 303/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 192.7502 - mse: 189.5549 - val_loss: 148.2052 - val_mse: 148.9225\n",
            "Epoch 304/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 191.2510 - mse: 188.2596 - val_loss: 146.7732 - val_mse: 147.4827\n",
            "Epoch 305/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 188.5347 - mse: 186.8244 - val_loss: 145.5729 - val_mse: 146.2774\n",
            "Epoch 306/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 175.7905 - mse: 185.5984 - val_loss: 144.4444 - val_mse: 145.1452\n",
            "Epoch 307/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 192.4794 - mse: 184.2839 - val_loss: 143.0537 - val_mse: 143.7463\n",
            "Epoch 308/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 193.0026 - mse: 182.9421 - val_loss: 141.9244 - val_mse: 142.6120\n",
            "Epoch 309/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 184.3266 - mse: 181.7707 - val_loss: 141.0723 - val_mse: 141.7587\n",
            "Epoch 310/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 181.7954 - mse: 180.4731 - val_loss: 140.0266 - val_mse: 140.7088\n",
            "Epoch 311/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 172.8367 - mse: 179.4727 - val_loss: 138.4607 - val_mse: 139.1315\n",
            "Epoch 312/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 183.3654 - mse: 178.1141 - val_loss: 137.3340 - val_mse: 137.9984\n",
            "Epoch 313/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 182.7288 - mse: 176.9661 - val_loss: 136.2588 - val_mse: 136.9180\n",
            "Epoch 314/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 155.3141 - mse: 175.7815 - val_loss: 135.4952 - val_mse: 136.1534\n",
            "Epoch 315/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 158.7221 - mse: 174.6342 - val_loss: 134.7927 - val_mse: 135.4499\n",
            "Epoch 316/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 164.9843 - mse: 173.6276 - val_loss: 133.5952 - val_mse: 134.2447\n",
            "Epoch 317/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 166.1007 - mse: 172.5359 - val_loss: 133.0330 - val_mse: 133.6827\n",
            "Epoch 318/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 175.5733 - mse: 171.4427 - val_loss: 131.8470 - val_mse: 132.4891\n",
            "Epoch 319/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 155.7872 - mse: 170.3443 - val_loss: 130.7003 - val_mse: 131.3347\n",
            "Epoch 320/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 173.6002 - mse: 169.3126 - val_loss: 129.7065 - val_mse: 130.3346\n",
            "Epoch 321/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 163.3954 - mse: 168.3433 - val_loss: 129.0518 - val_mse: 129.6783\n",
            "Epoch 322/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 173.3519 - mse: 167.4656 - val_loss: 127.8403 - val_mse: 128.4577\n",
            "Epoch 323/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 173.6127 - mse: 166.3940 - val_loss: 127.1443 - val_mse: 127.7596\n",
            "Epoch 324/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 163.1373 - mse: 165.3549 - val_loss: 126.3506 - val_mse: 126.9617\n",
            "Epoch 325/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 163.0092 - mse: 164.5171 - val_loss: 125.6276 - val_mse: 126.2355\n",
            "Epoch 326/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 153.5395 - mse: 163.5982 - val_loss: 124.5818 - val_mse: 125.1818\n",
            "Epoch 327/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 152.8219 - mse: 162.6983 - val_loss: 123.8945 - val_mse: 124.4912\n",
            "Epoch 328/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 154.2633 - mse: 161.7204 - val_loss: 123.1306 - val_mse: 123.7231\n",
            "Epoch 329/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 168.1629 - mse: 160.9677 - val_loss: 122.4542 - val_mse: 123.0434\n",
            "Epoch 330/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 149.9764 - mse: 160.0168 - val_loss: 121.8642 - val_mse: 122.4510\n",
            "Epoch 331/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 139.6877 - mse: 159.1899 - val_loss: 121.4166 - val_mse: 122.0026\n",
            "Epoch 332/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 164.1487 - mse: 158.3957 - val_loss: 120.5419 - val_mse: 121.1217\n",
            "Epoch 333/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 169.4697 - mse: 157.7145 - val_loss: 119.4414 - val_mse: 120.0116\n",
            "Epoch 334/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 150.8800 - mse: 156.8400 - val_loss: 119.1401 - val_mse: 119.7111\n",
            "Epoch 335/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 160.7949 - mse: 155.9190 - val_loss: 118.5145 - val_mse: 119.0818\n",
            "Epoch 336/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 155.6581 - mse: 155.1619 - val_loss: 117.5504 - val_mse: 118.1098\n",
            "Epoch 337/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 153.6425 - mse: 154.4510 - val_loss: 117.0321 - val_mse: 117.5890\n",
            "Epoch 338/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 152.7107 - mse: 153.7228 - val_loss: 116.5037 - val_mse: 117.0579\n",
            "Epoch 339/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 151.8830 - mse: 152.9026 - val_loss: 115.5442 - val_mse: 116.0900\n",
            "Epoch 340/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 162.1748 - mse: 152.1822 - val_loss: 114.8646 - val_mse: 115.4054\n",
            "Epoch 341/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 132.7855 - mse: 151.5142 - val_loss: 114.6635 - val_mse: 115.2053\n",
            "Epoch 342/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 150.3819 - mse: 150.8087 - val_loss: 113.8631 - val_mse: 114.3985\n",
            "Epoch 343/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 143.0131 - mse: 150.0896 - val_loss: 113.5224 - val_mse: 114.0571\n",
            "Epoch 344/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 155.1206 - mse: 149.3829 - val_loss: 112.9602 - val_mse: 113.4909\n",
            "Epoch 345/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 144.0612 - mse: 148.6569 - val_loss: 112.3568 - val_mse: 112.8832\n",
            "Epoch 346/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 152.0013 - mse: 148.0259 - val_loss: 111.7471 - val_mse: 112.2693\n",
            "Epoch 347/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 154.7945 - mse: 147.3468 - val_loss: 111.1934 - val_mse: 111.7115\n",
            "Epoch 348/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 153.2636 - mse: 146.7293 - val_loss: 110.7977 - val_mse: 111.3137\n",
            "Epoch 349/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 131.8809 - mse: 146.0578 - val_loss: 110.0615 - val_mse: 110.5712\n",
            "Epoch 350/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 149.5502 - mse: 145.4538 - val_loss: 109.5153 - val_mse: 110.0209\n",
            "Epoch 351/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 131.7315 - mse: 144.8381 - val_loss: 108.9069 - val_mse: 109.4073\n",
            "Epoch 352/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 146.4501 - mse: 144.3257 - val_loss: 108.9679 - val_mse: 109.4717\n",
            "Epoch 353/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 140.1884 - mse: 143.6977 - val_loss: 108.6964 - val_mse: 109.1992\n",
            "Epoch 354/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 145.5666 - mse: 143.0275 - val_loss: 107.9997 - val_mse: 108.4967\n",
            "Epoch 355/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 143.8107 - mse: 142.4840 - val_loss: 107.5423 - val_mse: 108.0360\n",
            "Epoch 356/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 142.5791 - mse: 141.7995 - val_loss: 106.9850 - val_mse: 107.4742\n",
            "Epoch 357/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 150.1008 - mse: 141.2776 - val_loss: 106.3249 - val_mse: 106.8085\n",
            "Epoch 358/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 145.4369 - mse: 140.7665 - val_loss: 105.5721 - val_mse: 106.0484\n",
            "Epoch 359/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 133.7043 - mse: 140.1152 - val_loss: 105.2315 - val_mse: 105.7058\n",
            "Epoch 360/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 142.1426 - mse: 139.7589 - val_loss: 105.2264 - val_mse: 105.7026\n",
            "Epoch 361/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 146.1556 - mse: 139.0418 - val_loss: 104.5996 - val_mse: 105.0700\n",
            "Epoch 362/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 138.1339 - mse: 138.5067 - val_loss: 104.1842 - val_mse: 104.6515\n",
            "Epoch 363/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 141.4813 - mse: 137.9786 - val_loss: 103.8601 - val_mse: 104.3254\n",
            "Epoch 364/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 138.4878 - mse: 137.4582 - val_loss: 103.5017 - val_mse: 103.9646\n",
            "Epoch 365/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 138.5182 - mse: 136.9596 - val_loss: 103.1060 - val_mse: 103.5659\n",
            "Epoch 366/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 138.5116 - mse: 136.4168 - val_loss: 102.6089 - val_mse: 103.0647\n",
            "Epoch 367/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 139.4322 - mse: 135.9764 - val_loss: 102.2596 - val_mse: 102.7127\n",
            "Epoch 368/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 138.4599 - mse: 135.4058 - val_loss: 102.0287 - val_mse: 102.4806\n",
            "Epoch 369/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 134.0080 - mse: 134.9606 - val_loss: 101.6960 - val_mse: 102.1454\n",
            "Epoch 370/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 117.7656 - mse: 134.4985 - val_loss: 100.9749 - val_mse: 101.4172\n",
            "Epoch 371/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 117.4022 - mse: 134.0889 - val_loss: 100.4649 - val_mse: 100.9026\n",
            "Epoch 372/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 126.1294 - mse: 133.4645 - val_loss: 100.4922 - val_mse: 100.9318\n",
            "Epoch 373/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 130.5625 - mse: 132.9634 - val_loss: 100.3996 - val_mse: 100.8397\n",
            "Epoch 374/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 135.0207 - mse: 132.5525 - val_loss: 100.0209 - val_mse: 100.4581\n",
            "Epoch 375/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 134.4284 - mse: 132.0613 - val_loss: 99.4428 - val_mse: 99.8748\n",
            "Epoch 376/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 131.2892 - mse: 131.5670 - val_loss: 99.1919 - val_mse: 99.6221\n",
            "Epoch 377/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 134.2489 - mse: 131.1433 - val_loss: 98.8619 - val_mse: 99.2896\n",
            "Epoch 378/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 125.0068 - mse: 130.6319 - val_loss: 98.6185 - val_mse: 99.0446\n",
            "Epoch 379/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 137.3160 - mse: 130.2646 - val_loss: 98.4134 - val_mse: 98.8386\n",
            "Epoch 380/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 132.5755 - mse: 129.7630 - val_loss: 97.8669 - val_mse: 98.2874\n",
            "Epoch 381/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 127.9436 - mse: 129.4026 - val_loss: 97.4827 - val_mse: 97.8998\n",
            "Epoch 382/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 128.8936 - mse: 128.9326 - val_loss: 97.4460 - val_mse: 97.8638\n",
            "Epoch 383/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 126.5267 - mse: 128.4351 - val_loss: 96.9821 - val_mse: 97.3959\n",
            "Epoch 384/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 130.6824 - mse: 128.0935 - val_loss: 96.2937 - val_mse: 96.7008\n",
            "Epoch 385/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 125.8530 - mse: 127.7002 - val_loss: 96.3583 - val_mse: 96.7676\n",
            "Epoch 386/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 127.5151 - mse: 127.1754 - val_loss: 95.7551 - val_mse: 96.1583\n",
            "Epoch 387/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 128.4464 - mse: 126.7793 - val_loss: 95.3120 - val_mse: 95.7113\n",
            "Epoch 388/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 132.2794 - mse: 126.3220 - val_loss: 95.1765 - val_mse: 95.5755\n",
            "Epoch 389/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 131.5772 - mse: 125.9370 - val_loss: 95.1099 - val_mse: 95.5094\n",
            "Epoch 390/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 126.9479 - mse: 125.5191 - val_loss: 94.9426 - val_mse: 95.3412\n",
            "Epoch 391/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 118.9182 - mse: 125.0486 - val_loss: 94.8316 - val_mse: 95.2298\n",
            "Epoch 392/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 123.9672 - mse: 124.7487 - val_loss: 94.5190 - val_mse: 94.9147\n",
            "Epoch 393/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 128.9302 - mse: 124.2882 - val_loss: 93.9209 - val_mse: 94.3113\n",
            "Epoch 394/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 116.0499 - mse: 123.8863 - val_loss: 93.5124 - val_mse: 93.8990\n",
            "Epoch 395/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 128.6151 - mse: 123.4156 - val_loss: 93.3770 - val_mse: 93.7633\n",
            "Epoch 396/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 123.9885 - mse: 123.1322 - val_loss: 93.4039 - val_mse: 93.7919\n",
            "Epoch 397/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 127.9385 - mse: 122.6559 - val_loss: 93.0288 - val_mse: 93.4135\n",
            "Epoch 398/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 128.0510 - mse: 122.2938 - val_loss: 92.5585 - val_mse: 92.9392\n",
            "Epoch 399/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 126.9941 - mse: 121.8534 - val_loss: 92.2161 - val_mse: 92.5940\n",
            "Epoch 400/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 120.7180 - mse: 121.4540 - val_loss: 92.0907 - val_mse: 92.4680\n",
            "Epoch 401/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 128.4241 - mse: 121.0807 - val_loss: 91.7920 - val_mse: 92.1669\n",
            "Epoch 402/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 120.4199 - mse: 120.7417 - val_loss: 91.6058 - val_mse: 91.9800\n",
            "Epoch 403/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 124.1348 - mse: 120.3303 - val_loss: 91.1455 - val_mse: 91.5156\n",
            "Epoch 404/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 116.3946 - mse: 119.9024 - val_loss: 91.0274 - val_mse: 91.3972\n",
            "Epoch 405/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 116.6373 - mse: 119.5674 - val_loss: 90.8273 - val_mse: 91.1958\n",
            "Epoch 406/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 121.7041 - mse: 119.3013 - val_loss: 90.8868 - val_mse: 91.2570\n",
            "Epoch 407/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 123.4256 - mse: 118.7985 - val_loss: 90.3273 - val_mse: 90.6926\n",
            "Epoch 408/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 116.4028 - mse: 118.5943 - val_loss: 90.3250 - val_mse: 90.6913\n",
            "Epoch 409/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 119.0671 - mse: 118.0686 - val_loss: 89.5196 - val_mse: 89.8781\n",
            "Epoch 410/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 115.7234 - mse: 117.9159 - val_loss: 89.6563 - val_mse: 90.0174\n",
            "Epoch 411/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 113.3838 - mse: 117.2927 - val_loss: 88.9853 - val_mse: 89.3403\n",
            "Epoch 412/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 117.8219 - mse: 116.9552 - val_loss: 88.7190 - val_mse: 89.0721\n",
            "Epoch 413/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 111.2779 - mse: 116.5681 - val_loss: 88.5655 - val_mse: 88.9179\n",
            "Epoch 414/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 122.0783 - mse: 116.2186 - val_loss: 88.3185 - val_mse: 88.6691\n",
            "Epoch 415/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 116.1101 - mse: 115.8584 - val_loss: 88.1421 - val_mse: 88.4918\n",
            "Epoch 416/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 116.3749 - mse: 115.5683 - val_loss: 87.9238 - val_mse: 88.2718\n",
            "Epoch 417/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 119.8908 - mse: 115.1934 - val_loss: 87.4146 - val_mse: 87.7584\n",
            "Epoch 418/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 110.9130 - mse: 114.8821 - val_loss: 87.2748 - val_mse: 87.6181\n",
            "Epoch 419/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 117.7785 - mse: 114.4265 - val_loss: 87.2596 - val_mse: 87.6041\n",
            "Epoch 420/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 115.1547 - mse: 114.2404 - val_loss: 87.2051 - val_mse: 87.5500\n",
            "Epoch 421/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 118.1208 - mse: 113.7463 - val_loss: 87.0054 - val_mse: 87.3494\n",
            "Epoch 422/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 114.3861 - mse: 113.3848 - val_loss: 86.5962 - val_mse: 86.9368\n",
            "Epoch 423/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 116.4087 - mse: 113.0459 - val_loss: 86.4467 - val_mse: 86.7866\n",
            "Epoch 424/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 110.0226 - mse: 112.7783 - val_loss: 85.8148 - val_mse: 86.1487\n",
            "Epoch 425/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 112.4382 - mse: 112.3652 - val_loss: 85.6180 - val_mse: 85.9511\n",
            "Epoch 426/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 101.3175 - mse: 111.9886 - val_loss: 85.7652 - val_mse: 86.1007\n",
            "Epoch 427/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 106.2804 - mse: 111.6803 - val_loss: 85.6933 - val_mse: 86.0293\n",
            "Epoch 428/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 111.8827 - mse: 111.3246 - val_loss: 85.5467 - val_mse: 85.8823\n",
            "Epoch 429/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 109.8784 - mse: 111.0281 - val_loss: 84.8168 - val_mse: 85.1458\n",
            "Epoch 430/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 113.9191 - mse: 110.6996 - val_loss: 84.2616 - val_mse: 84.5858\n",
            "Epoch 431/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 109.9472 - mse: 110.2962 - val_loss: 84.4813 - val_mse: 84.8088\n",
            "Epoch 432/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 106.1432 - mse: 109.9257 - val_loss: 84.0787 - val_mse: 84.4029\n",
            "Epoch 433/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 112.4038 - mse: 109.7640 - val_loss: 84.3161 - val_mse: 84.6441\n",
            "Epoch 434/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 106.5452 - mse: 109.2209 - val_loss: 83.7270 - val_mse: 84.0501\n",
            "Epoch 435/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 111.9664 - mse: 108.9247 - val_loss: 83.5846 - val_mse: 83.9072\n",
            "Epoch 436/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 97.8916 - mse: 108.5221 - val_loss: 83.3836 - val_mse: 83.7049\n",
            "Epoch 437/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 109.1860 - mse: 108.3240 - val_loss: 82.7682 - val_mse: 83.0838\n",
            "Epoch 438/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 113.7281 - mse: 107.9045 - val_loss: 82.6205 - val_mse: 82.9358\n",
            "Epoch 439/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 108.2291 - mse: 107.7282 - val_loss: 82.9033 - val_mse: 83.2231\n",
            "Epoch 440/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 113.2663 - mse: 107.3677 - val_loss: 82.1548 - val_mse: 82.4680\n",
            "Epoch 441/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 105.8087 - mse: 106.9247 - val_loss: 82.2921 - val_mse: 82.6077\n",
            "Epoch 442/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 111.9984 - mse: 106.6029 - val_loss: 82.0647 - val_mse: 82.3793\n",
            "Epoch 443/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 106.8307 - mse: 106.2097 - val_loss: 81.8142 - val_mse: 82.1273\n",
            "Epoch 444/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 108.8509 - mse: 105.9298 - val_loss: 81.4302 - val_mse: 81.7401\n",
            "Epoch 445/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 107.8385 - mse: 105.5953 - val_loss: 81.3167 - val_mse: 81.6266\n",
            "Epoch 446/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 106.6854 - mse: 105.3834 - val_loss: 80.8079 - val_mse: 81.1133\n",
            "Epoch 447/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 106.9821 - mse: 104.9633 - val_loss: 80.8488 - val_mse: 81.1560\n",
            "Epoch 448/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 109.6364 - mse: 104.6633 - val_loss: 80.7154 - val_mse: 81.0223\n",
            "Epoch 449/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 97.8260 - mse: 104.3369 - val_loss: 80.2903 - val_mse: 80.5936\n",
            "Epoch 450/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 104.0568 - mse: 104.0303 - val_loss: 80.4031 - val_mse: 80.7086\n",
            "Epoch 451/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 97.1371 - mse: 103.6525 - val_loss: 80.0570 - val_mse: 80.3605\n",
            "Epoch 452/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 102.5488 - mse: 103.3731 - val_loss: 79.8370 - val_mse: 80.1393\n",
            "Epoch 453/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 105.8096 - mse: 103.0201 - val_loss: 79.5003 - val_mse: 79.8004\n",
            "Epoch 454/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 106.7340 - mse: 102.6931 - val_loss: 79.4044 - val_mse: 79.7047\n",
            "Epoch 455/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 95.2628 - mse: 102.3889 - val_loss: 79.2186 - val_mse: 79.5179\n",
            "Epoch 456/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.1547 - mse: 102.2190 - val_loss: 78.7203 - val_mse: 79.0152\n",
            "Epoch 457/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 94.0504 - mse: 101.8021 - val_loss: 78.8153 - val_mse: 79.1124\n",
            "Epoch 458/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 101.4644 - mse: 101.4804 - val_loss: 78.8388 - val_mse: 79.1373\n",
            "Epoch 459/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 95.7952 - mse: 101.1681 - val_loss: 78.6227 - val_mse: 78.9199\n",
            "Epoch 460/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 100.3348 - mse: 100.7975 - val_loss: 78.2107 - val_mse: 78.5051\n",
            "Epoch 461/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 97.1775 - mse: 100.6039 - val_loss: 77.4864 - val_mse: 77.7745\n",
            "Epoch 462/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 103.5529 - mse: 100.2315 - val_loss: 77.2237 - val_mse: 77.5103\n",
            "Epoch 463/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 100.8982 - mse: 99.9316 - val_loss: 77.5119 - val_mse: 77.8027\n",
            "Epoch 464/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 104.2229 - mse: 99.7161 - val_loss: 77.5853 - val_mse: 77.8781\n",
            "Epoch 465/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 93.1316 - mse: 99.3420 - val_loss: 77.4293 - val_mse: 77.7216\n",
            "Epoch 466/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 106.3555 - mse: 99.1593 - val_loss: 76.5217 - val_mse: 76.8058\n",
            "Epoch 467/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.4090 - mse: 98.7362 - val_loss: 76.2212 - val_mse: 76.5029\n",
            "Epoch 468/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 101.1094 - mse: 98.3912 - val_loss: 76.4365 - val_mse: 76.7220\n",
            "Epoch 469/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.7240 - mse: 98.0775 - val_loss: 76.2068 - val_mse: 76.4912\n",
            "Epoch 470/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.5822 - mse: 97.7726 - val_loss: 76.2943 - val_mse: 76.5806\n",
            "Epoch 471/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 94.6841 - mse: 97.5101 - val_loss: 76.1715 - val_mse: 76.4576\n",
            "Epoch 472/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 99.8286 - mse: 97.1898 - val_loss: 75.6356 - val_mse: 75.9178\n",
            "Epoch 473/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 97.2370 - mse: 96.8486 - val_loss: 75.3324 - val_mse: 75.6125\n",
            "Epoch 474/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 102.7254 - mse: 96.6887 - val_loss: 74.8390 - val_mse: 75.1150\n",
            "Epoch 475/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 95.9116 - mse: 96.3207 - val_loss: 75.0085 - val_mse: 75.2875\n",
            "Epoch 476/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 94.0204 - mse: 96.0417 - val_loss: 74.6974 - val_mse: 74.9742\n",
            "Epoch 477/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 97.2036 - mse: 95.7193 - val_loss: 74.5681 - val_mse: 74.8449\n",
            "Epoch 478/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 91.2221 - mse: 95.5589 - val_loss: 74.8534 - val_mse: 75.1338\n",
            "Epoch 479/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.4970 - mse: 95.0907 - val_loss: 74.3742 - val_mse: 74.6512\n",
            "Epoch 480/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 89.8540 - mse: 94.8978 - val_loss: 73.7479 - val_mse: 74.0196\n",
            "Epoch 481/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 94.7654 - mse: 94.5534 - val_loss: 73.7788 - val_mse: 74.0521\n",
            "Epoch 482/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 91.5699 - mse: 94.2658 - val_loss: 73.4051 - val_mse: 73.6753\n",
            "Epoch 483/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.5386 - mse: 94.0071 - val_loss: 73.3215 - val_mse: 73.5923\n",
            "Epoch 484/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 91.5329 - mse: 93.6080 - val_loss: 73.2402 - val_mse: 73.5110\n",
            "Epoch 485/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 96.4031 - mse: 93.3982 - val_loss: 73.0387 - val_mse: 73.3086\n",
            "Epoch 486/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 89.5170 - mse: 93.0766 - val_loss: 72.8753 - val_mse: 73.1446\n",
            "Epoch 487/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 93.3817 - mse: 92.8439 - val_loss: 72.5218 - val_mse: 72.7887\n",
            "Epoch 488/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 92.8506 - mse: 92.7420 - val_loss: 72.8002 - val_mse: 73.0708\n",
            "Epoch 489/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 92.7041 - mse: 92.2294 - val_loss: 72.3711 - val_mse: 72.6389\n",
            "Epoch 490/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 92.4946 - mse: 91.9205 - val_loss: 72.0171 - val_mse: 72.2826\n",
            "Epoch 491/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 92.7248 - mse: 91.7219 - val_loss: 71.4819 - val_mse: 71.7428\n",
            "Epoch 492/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 92.3451 - mse: 91.4116 - val_loss: 71.2663 - val_mse: 71.5261\n",
            "Epoch 493/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 90.1325 - mse: 91.0596 - val_loss: 71.3799 - val_mse: 71.6421\n",
            "Epoch 494/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 93.4934 - mse: 90.7708 - val_loss: 71.3394 - val_mse: 71.6021\n",
            "Epoch 495/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 85.3053 - mse: 90.5659 - val_loss: 71.4367 - val_mse: 71.7014\n",
            "Epoch 496/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 85.5452 - mse: 90.3157 - val_loss: 71.1413 - val_mse: 71.4043\n",
            "Epoch 497/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 93.8023 - mse: 89.9840 - val_loss: 70.7516 - val_mse: 71.0120\n",
            "Epoch 498/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 93.2810 - mse: 89.9002 - val_loss: 70.1207 - val_mse: 70.3754\n",
            "Epoch 499/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 86.1918 - mse: 89.4738 - val_loss: 70.1228 - val_mse: 70.3786\n",
            "Epoch 500/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 89.7625 - mse: 89.3023 - val_loss: 70.3326 - val_mse: 70.5917\n",
            "Epoch 501/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 89.5528 - mse: 88.9416 - val_loss: 69.7935 - val_mse: 70.0486\n",
            "Epoch 502/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 93.3409 - mse: 88.6411 - val_loss: 69.6352 - val_mse: 69.8895\n",
            "Epoch 503/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 81.3058 - mse: 88.3884 - val_loss: 69.7658 - val_mse: 70.0226\n",
            "Epoch 504/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 89.1257 - mse: 88.1087 - val_loss: 69.2517 - val_mse: 69.5045\n",
            "Epoch 505/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 91.2365 - mse: 87.9755 - val_loss: 68.8069 - val_mse: 69.0559\n",
            "Epoch 506/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 87.0640 - mse: 87.5318 - val_loss: 69.0582 - val_mse: 69.3108\n",
            "Epoch 507/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 89.8315 - mse: 87.2880 - val_loss: 68.9279 - val_mse: 69.1803\n",
            "Epoch 508/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 91.4527 - mse: 87.0263 - val_loss: 68.5854 - val_mse: 68.8355\n",
            "Epoch 509/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 84.0566 - mse: 86.7583 - val_loss: 68.5988 - val_mse: 68.8498\n",
            "Epoch 510/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 90.2227 - mse: 86.4580 - val_loss: 68.3121 - val_mse: 68.5615\n",
            "Epoch 511/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 83.5422 - mse: 86.2565 - val_loss: 68.0644 - val_mse: 68.3123\n",
            "Epoch 512/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 85.3530 - mse: 85.9558 - val_loss: 67.9451 - val_mse: 68.1927\n",
            "Epoch 513/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 85.7919 - mse: 85.7369 - val_loss: 67.6477 - val_mse: 67.8933\n",
            "Epoch 514/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 81.9555 - mse: 85.4948 - val_loss: 67.7568 - val_mse: 68.0043\n",
            "Epoch 515/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 84.8141 - mse: 85.1548 - val_loss: 67.3890 - val_mse: 67.6339\n",
            "Epoch 516/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 83.9244 - mse: 84.9864 - val_loss: 66.9382 - val_mse: 67.1800\n",
            "Epoch 517/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 88.9859 - mse: 84.6631 - val_loss: 66.8704 - val_mse: 67.1125\n",
            "Epoch 518/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 89.8520 - mse: 84.3884 - val_loss: 66.6460 - val_mse: 66.8867\n",
            "Epoch 519/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 83.7395 - mse: 84.1383 - val_loss: 66.5732 - val_mse: 66.8141\n",
            "Epoch 520/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 86.1660 - mse: 83.9414 - val_loss: 66.5748 - val_mse: 66.8167\n",
            "Epoch 521/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 81.6696 - mse: 83.7429 - val_loss: 66.5472 - val_mse: 66.7899\n",
            "Epoch 522/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 87.5864 - mse: 83.3459 - val_loss: 65.9811 - val_mse: 66.2194\n",
            "Epoch 523/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 80.1502 - mse: 83.2424 - val_loss: 65.9093 - val_mse: 66.1477\n",
            "Epoch 524/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 82.3560 - mse: 82.8759 - val_loss: 65.5864 - val_mse: 65.8223\n",
            "Epoch 525/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 75.8123 - mse: 82.6136 - val_loss: 65.2950 - val_mse: 65.5287\n",
            "Epoch 526/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 86.4740 - mse: 82.4351 - val_loss: 65.0192 - val_mse: 65.2514\n",
            "Epoch 527/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 84.0938 - mse: 82.1304 - val_loss: 64.9775 - val_mse: 65.2104\n",
            "Epoch 528/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 80.4776 - mse: 81.9368 - val_loss: 65.0998 - val_mse: 65.3347\n",
            "Epoch 529/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 81.5008 - mse: 81.7085 - val_loss: 64.6555 - val_mse: 64.8870\n",
            "Epoch 530/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 77.5146 - mse: 81.4138 - val_loss: 64.6192 - val_mse: 64.8513\n",
            "Epoch 531/600\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 83.8677 - mse: 81.2192 - val_loss: 64.5501 - val_mse: 64.7827\n",
            "Epoch 532/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 83.3276 - mse: 81.0543 - val_loss: 64.7780 - val_mse: 65.0134\n",
            "Epoch 533/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 81.9018 - mse: 80.7025 - val_loss: 64.3216 - val_mse: 64.5538\n",
            "Epoch 534/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 78.5760 - mse: 80.4484 - val_loss: 63.9991 - val_mse: 64.2289\n",
            "Epoch 535/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 76.5862 - mse: 80.2113 - val_loss: 63.8194 - val_mse: 64.0483\n",
            "Epoch 536/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 81.9409 - mse: 79.9867 - val_loss: 63.5984 - val_mse: 63.8261\n",
            "Epoch 537/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 76.4452 - mse: 79.8002 - val_loss: 63.2197 - val_mse: 63.4446\n",
            "Epoch 538/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 77.1574 - mse: 79.4790 - val_loss: 63.2409 - val_mse: 63.4671\n",
            "Epoch 539/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 77.5790 - mse: 79.2561 - val_loss: 63.0637 - val_mse: 63.2889\n",
            "Epoch 540/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 82.2957 - mse: 79.0342 - val_loss: 62.9949 - val_mse: 63.2207\n",
            "Epoch 541/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 81.3298 - mse: 78.9179 - val_loss: 62.6906 - val_mse: 62.9144\n",
            "Epoch 542/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 80.7555 - mse: 78.5346 - val_loss: 62.6815 - val_mse: 62.9060\n",
            "Epoch 543/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 79.6960 - mse: 78.3305 - val_loss: 62.4303 - val_mse: 62.6531\n",
            "Epoch 544/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 78.7452 - mse: 78.1537 - val_loss: 62.7188 - val_mse: 62.9447\n",
            "Epoch 545/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 79.7600 - mse: 77.9135 - val_loss: 62.2706 - val_mse: 62.4932\n",
            "Epoch 546/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 77.5159 - mse: 77.6157 - val_loss: 62.0527 - val_mse: 62.2741\n",
            "Epoch 547/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 78.9112 - mse: 77.4322 - val_loss: 61.9443 - val_mse: 62.1655\n",
            "Epoch 548/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 76.6042 - mse: 77.1869 - val_loss: 61.7949 - val_mse: 62.0156\n",
            "Epoch 549/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 80.1280 - mse: 77.0115 - val_loss: 61.6183 - val_mse: 61.8380\n",
            "Epoch 550/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 73.1895 - mse: 76.7638 - val_loss: 61.1790 - val_mse: 61.3956\n",
            "Epoch 551/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 76.0011 - mse: 76.5429 - val_loss: 60.8791 - val_mse: 61.0933\n",
            "Epoch 552/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 78.5008 - mse: 76.2859 - val_loss: 60.7796 - val_mse: 60.9937\n",
            "Epoch 553/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 79.7927 - mse: 76.0138 - val_loss: 60.9431 - val_mse: 61.1597\n",
            "Epoch 554/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 73.4732 - mse: 75.8423 - val_loss: 61.1428 - val_mse: 61.3617\n",
            "Epoch 555/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 76.3965 - mse: 75.7348 - val_loss: 61.0903 - val_mse: 61.3092\n",
            "Epoch 556/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 75.6582 - mse: 75.3743 - val_loss: 60.3631 - val_mse: 60.5765\n",
            "Epoch 557/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 79.7392 - mse: 75.1605 - val_loss: 60.1266 - val_mse: 60.3386\n",
            "Epoch 558/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.3718 - mse: 74.9541 - val_loss: 59.9882 - val_mse: 60.1995\n",
            "Epoch 559/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 76.1988 - mse: 74.7393 - val_loss: 59.7951 - val_mse: 60.0052\n",
            "Epoch 560/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.6094 - mse: 74.5006 - val_loss: 59.6547 - val_mse: 59.8642\n",
            "Epoch 561/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 71.6290 - mse: 74.3263 - val_loss: 59.6914 - val_mse: 59.9019\n",
            "Epoch 562/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 70.9690 - mse: 74.0883 - val_loss: 59.3916 - val_mse: 59.6000\n",
            "Epoch 563/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 75.4429 - mse: 73.8787 - val_loss: 59.2192 - val_mse: 59.4270\n",
            "Epoch 564/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.3595 - mse: 73.6613 - val_loss: 59.1332 - val_mse: 59.3407\n",
            "Epoch 565/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 75.6883 - mse: 73.4519 - val_loss: 58.9542 - val_mse: 59.1608\n",
            "Epoch 566/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.4158 - mse: 73.3651 - val_loss: 59.0784 - val_mse: 59.2868\n",
            "Epoch 567/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 75.6898 - mse: 73.1308 - val_loss: 58.6658 - val_mse: 58.8714\n",
            "Epoch 568/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 72.4227 - mse: 72.8895 - val_loss: 58.7149 - val_mse: 58.9214\n",
            "Epoch 569/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 74.2438 - mse: 72.6210 - val_loss: 58.2690 - val_mse: 58.4720\n",
            "Epoch 570/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 66.9610 - mse: 72.3981 - val_loss: 58.0813 - val_mse: 58.2832\n",
            "Epoch 571/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 73.4094 - mse: 72.2120 - val_loss: 58.1028 - val_mse: 58.3055\n",
            "Epoch 572/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 74.3232 - mse: 72.0866 - val_loss: 57.8755 - val_mse: 58.0768\n",
            "Epoch 573/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 73.3278 - mse: 71.9189 - val_loss: 57.7437 - val_mse: 57.9444\n",
            "Epoch 574/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.5829 - mse: 71.6059 - val_loss: 57.7747 - val_mse: 57.9762\n",
            "Epoch 575/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 75.3052 - mse: 71.4086 - val_loss: 57.6083 - val_mse: 57.8089\n",
            "Epoch 576/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 68.7324 - mse: 71.3088 - val_loss: 57.7052 - val_mse: 57.9070\n",
            "Epoch 577/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 73.9394 - mse: 70.9876 - val_loss: 57.1952 - val_mse: 57.3933\n",
            "Epoch 578/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 69.8253 - mse: 70.7845 - val_loss: 56.8954 - val_mse: 57.0912\n",
            "Epoch 579/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 74.0618 - mse: 70.6286 - val_loss: 56.9194 - val_mse: 57.1164\n",
            "Epoch 580/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 67.8086 - mse: 70.5231 - val_loss: 57.0936 - val_mse: 57.2925\n",
            "Epoch 581/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 67.4777 - mse: 70.1976 - val_loss: 56.7969 - val_mse: 56.9937\n",
            "Epoch 582/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 66.1010 - mse: 69.9926 - val_loss: 56.4497 - val_mse: 56.6438\n",
            "Epoch 583/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 68.1123 - mse: 70.1103 - val_loss: 56.0350 - val_mse: 56.2259\n",
            "Epoch 584/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 70.9971 - mse: 69.6542 - val_loss: 56.1812 - val_mse: 56.3741\n",
            "Epoch 585/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 71.6128 - mse: 69.5113 - val_loss: 56.3125 - val_mse: 56.5071\n",
            "Epoch 586/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 68.8479 - mse: 69.2655 - val_loss: 55.8989 - val_mse: 56.0904\n",
            "Epoch 587/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 68.3994 - mse: 69.0418 - val_loss: 55.7364 - val_mse: 55.9272\n",
            "Epoch 588/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 69.5591 - mse: 68.8773 - val_loss: 55.6779 - val_mse: 55.8688\n",
            "Epoch 589/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 64.2976 - mse: 68.6452 - val_loss: 55.6446 - val_mse: 55.8356\n",
            "Epoch 590/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 70.6712 - mse: 68.5624 - val_loss: 55.3277 - val_mse: 55.5166\n",
            "Epoch 591/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.5560 - mse: 68.3163 - val_loss: 55.2555 - val_mse: 55.4443\n",
            "Epoch 592/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 70.3198 - mse: 68.1546 - val_loss: 55.4334 - val_mse: 55.6239\n",
            "Epoch 593/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 66.1953 - mse: 68.1348 - val_loss: 54.8582 - val_mse: 55.0444\n",
            "Epoch 594/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 69.9122 - mse: 67.7487 - val_loss: 55.0064 - val_mse: 55.1944\n",
            "Epoch 595/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 69.0106 - mse: 67.6301 - val_loss: 54.9708 - val_mse: 55.1589\n",
            "Epoch 596/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 72.0026 - mse: 67.6140 - val_loss: 54.3971 - val_mse: 54.5809\n",
            "Epoch 597/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 66.1686 - mse: 67.1886 - val_loss: 54.4675 - val_mse: 54.6521\n",
            "Epoch 598/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 64.6531 - mse: 67.0203 - val_loss: 54.3111 - val_mse: 54.4950\n",
            "Epoch 599/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 68.0211 - mse: 66.8230 - val_loss: 54.3391 - val_mse: 54.5236\n",
            "Epoch 600/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 61.0732 - mse: 66.7070 - val_loss: 54.3495 - val_mse: 54.5342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2dea30eac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbpchQ5CSpqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  tf.keras.layers.Dense(3,activation='sigmoid'),\n",
        "  \n",
        "  tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecakWUcEU2jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfGqLPq3Sp4w",
        "colab_type": "code",
        "outputId": "078ba64e-742e-4aba-e3bc-45f082c0150f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=600)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/600\n",
            "9/9 [==============================] - 1s 96ms/step - loss: 571.5037 - mse: 571.8893 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 561.2301 - mse: 571.0004 - val_loss: 609.2463 - val_mse: 609.5320\n",
            "Epoch 3/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 571.0575 - mse: 570.1465 - val_loss: 608.4199 - val_mse: 608.7056\n",
            "Epoch 4/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 567.9314 - mse: 569.3408 - val_loss: 607.6010 - val_mse: 607.8865\n",
            "Epoch 5/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 578.1251 - mse: 568.5544 - val_loss: 606.7810 - val_mse: 607.0664\n",
            "Epoch 6/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 573.0108 - mse: 567.7762 - val_loss: 605.9578 - val_mse: 606.2431\n",
            "Epoch 7/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 572.9218 - mse: 566.9819 - val_loss: 605.1430 - val_mse: 605.4283\n",
            "Epoch 8/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 575.7015 - mse: 566.1901 - val_loss: 604.3335 - val_mse: 604.6187\n",
            "Epoch 9/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 573.5099 - mse: 565.4188 - val_loss: 603.5146 - val_mse: 603.7996\n",
            "Epoch 10/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 565.3742 - mse: 564.6275 - val_loss: 602.7034 - val_mse: 602.9883\n",
            "Epoch 11/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 566.2150 - mse: 563.8571 - val_loss: 601.8844 - val_mse: 602.1693\n",
            "Epoch 12/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 573.0504 - mse: 563.0728 - val_loss: 601.0701 - val_mse: 601.3549\n",
            "Epoch 13/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 565.6303 - mse: 562.2834 - val_loss: 600.2650 - val_mse: 600.5497\n",
            "Epoch 14/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 566.4923 - mse: 561.5161 - val_loss: 599.4506 - val_mse: 599.7352\n",
            "Epoch 15/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 563.6320 - mse: 560.7249 - val_loss: 598.6476 - val_mse: 598.9321\n",
            "Epoch 16/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 546.8964 - mse: 559.9532 - val_loss: 597.8402 - val_mse: 598.1247\n",
            "Epoch 17/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 562.1997 - mse: 559.1823 - val_loss: 597.0280 - val_mse: 597.3123\n",
            "Epoch 18/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 567.2259 - mse: 558.4072 - val_loss: 596.2172 - val_mse: 596.5014\n",
            "Epoch 19/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 558.7778 - mse: 557.6325 - val_loss: 595.4087 - val_mse: 595.6929\n",
            "Epoch 20/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 563.2517 - mse: 556.8554 - val_loss: 594.6046 - val_mse: 594.8887\n",
            "Epoch 21/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 563.3958 - mse: 556.0861 - val_loss: 593.7994 - val_mse: 594.0834\n",
            "Epoch 22/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 551.9359 - mse: 555.3049 - val_loss: 593.0038 - val_mse: 593.2877\n",
            "Epoch 23/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 551.4991 - mse: 554.5417 - val_loss: 592.2016 - val_mse: 592.4854\n",
            "Epoch 24/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 556.4807 - mse: 553.7734 - val_loss: 591.3987 - val_mse: 591.6824\n",
            "Epoch 25/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 567.4402 - mse: 553.0053 - val_loss: 590.5964 - val_mse: 590.8800\n",
            "Epoch 26/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 549.5084 - mse: 552.2380 - val_loss: 589.7958 - val_mse: 590.0793\n",
            "Epoch 27/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 554.8703 - mse: 551.4709 - val_loss: 588.9965 - val_mse: 589.2800\n",
            "Epoch 28/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 546.1523 - mse: 550.6959 - val_loss: 588.2063 - val_mse: 588.4896\n",
            "Epoch 29/600\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 557.4711 - mse: 549.9418 - val_loss: 587.4064 - val_mse: 587.6896\n",
            "Epoch 30/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 536.2798 - mse: 549.1750 - val_loss: 586.6104 - val_mse: 586.8935\n",
            "Epoch 31/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 545.1036 - mse: 548.4127 - val_loss: 585.8136 - val_mse: 586.0967\n",
            "Epoch 32/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 539.3888 - mse: 547.6451 - val_loss: 585.0222 - val_mse: 585.3051\n",
            "Epoch 33/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 553.1558 - mse: 546.8893 - val_loss: 584.2264 - val_mse: 584.5093\n",
            "Epoch 34/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 545.8257 - mse: 546.1261 - val_loss: 583.4342 - val_mse: 583.7170\n",
            "Epoch 35/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 547.5008 - mse: 545.3606 - val_loss: 582.6477 - val_mse: 582.9304\n",
            "Epoch 36/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 554.3711 - mse: 544.6100 - val_loss: 581.8551 - val_mse: 582.1377\n",
            "Epoch 37/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 540.7433 - mse: 543.8486 - val_loss: 581.0671 - val_mse: 581.3497\n",
            "Epoch 38/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 532.8509 - mse: 543.0949 - val_loss: 580.2769 - val_mse: 580.5593\n",
            "Epoch 39/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 547.3574 - mse: 542.3380 - val_loss: 579.4877 - val_mse: 579.7700\n",
            "Epoch 40/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 550.9187 - mse: 541.5841 - val_loss: 578.6990 - val_mse: 578.9813\n",
            "Epoch 41/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 531.1095 - mse: 540.8265 - val_loss: 577.9151 - val_mse: 578.1973\n",
            "Epoch 42/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 535.7683 - mse: 540.0758 - val_loss: 577.1297 - val_mse: 577.4118\n",
            "Epoch 43/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 542.4287 - mse: 539.3253 - val_loss: 576.3440 - val_mse: 576.6260\n",
            "Epoch 44/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 536.7675 - mse: 538.5676 - val_loss: 575.5650 - val_mse: 575.8469\n",
            "Epoch 45/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 541.3447 - mse: 537.8231 - val_loss: 574.7816 - val_mse: 575.0633\n",
            "Epoch 46/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 533.6340 - mse: 537.0724 - val_loss: 574.0005 - val_mse: 574.2822\n",
            "Epoch 47/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 538.8094 - mse: 536.3254 - val_loss: 573.2191 - val_mse: 573.5007\n",
            "Epoch 48/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 536.5335 - mse: 535.5728 - val_loss: 572.4431 - val_mse: 572.7246\n",
            "Epoch 49/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 534.2571 - mse: 534.8276 - val_loss: 571.6660 - val_mse: 571.9474\n",
            "Epoch 50/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 533.1455 - mse: 534.0843 - val_loss: 570.8870 - val_mse: 571.1683\n",
            "Epoch 51/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 531.4788 - mse: 533.3342 - val_loss: 570.1132 - val_mse: 570.3945\n",
            "Epoch 52/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 535.0103 - mse: 532.5934 - val_loss: 569.3367 - val_mse: 569.6179\n",
            "Epoch 53/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 517.0172 - mse: 531.8484 - val_loss: 568.5625 - val_mse: 568.8436\n",
            "Epoch 54/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 518.3439 - mse: 531.1137 - val_loss: 567.7818 - val_mse: 568.0628\n",
            "Epoch 55/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 546.5444 - mse: 530.3721 - val_loss: 567.0020 - val_mse: 567.2828\n",
            "Epoch 56/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 524.8516 - mse: 529.6141 - val_loss: 566.2397 - val_mse: 566.5204\n",
            "Epoch 57/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 534.6359 - mse: 528.8827 - val_loss: 565.4706 - val_mse: 565.7513\n",
            "Epoch 58/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 501.4046 - mse: 528.1412 - val_loss: 564.7047 - val_mse: 564.9853\n",
            "Epoch 59/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 527.9826 - mse: 527.4139 - val_loss: 563.9285 - val_mse: 564.2090\n",
            "Epoch 60/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 525.5484 - mse: 526.6783 - val_loss: 563.1528 - val_mse: 563.4332\n",
            "Epoch 61/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 512.3301 - mse: 525.9277 - val_loss: 562.3907 - val_mse: 562.6711\n",
            "Epoch 62/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 521.3756 - mse: 525.1951 - val_loss: 561.6261 - val_mse: 561.9063\n",
            "Epoch 63/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 525.0200 - mse: 524.4638 - val_loss: 560.8596 - val_mse: 561.1398\n",
            "Epoch 64/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 535.8300 - mse: 523.7375 - val_loss: 560.0881 - val_mse: 560.3682\n",
            "Epoch 65/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 520.6168 - mse: 522.9897 - val_loss: 559.3319 - val_mse: 559.6119\n",
            "Epoch 66/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 522.9347 - mse: 522.2642 - val_loss: 558.5705 - val_mse: 558.8503\n",
            "Epoch 67/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 512.8250 - mse: 521.5403 - val_loss: 557.8038 - val_mse: 558.0836\n",
            "Epoch 68/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 539.3206 - mse: 520.8069 - val_loss: 557.0407 - val_mse: 557.3204\n",
            "Epoch 69/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 522.1289 - mse: 520.0707 - val_loss: 556.2866 - val_mse: 556.5662\n",
            "Epoch 70/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 521.5465 - mse: 519.3502 - val_loss: 555.5264 - val_mse: 555.8059\n",
            "Epoch 71/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 516.6630 - mse: 518.6238 - val_loss: 554.7667 - val_mse: 555.0462\n",
            "Epoch 72/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 526.3532 - mse: 517.8925 - val_loss: 554.0120 - val_mse: 554.2914\n",
            "Epoch 73/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 514.2745 - mse: 517.1740 - val_loss: 553.2521 - val_mse: 553.5313\n",
            "Epoch 74/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 508.9181 - mse: 516.4406 - val_loss: 552.5005 - val_mse: 552.7797\n",
            "Epoch 75/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 503.4480 - mse: 515.7176 - val_loss: 551.7480 - val_mse: 552.0272\n",
            "Epoch 76/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 520.5629 - mse: 514.9985 - val_loss: 550.9921 - val_mse: 551.2711\n",
            "Epoch 77/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 521.3682 - mse: 514.2816 - val_loss: 550.2330 - val_mse: 550.5119\n",
            "Epoch 78/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 512.9752 - mse: 513.5538 - val_loss: 549.4813 - val_mse: 549.7601\n",
            "Epoch 79/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 515.7371 - mse: 512.8351 - val_loss: 548.7285 - val_mse: 549.0072\n",
            "Epoch 80/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 516.5567 - mse: 512.1142 - val_loss: 547.9780 - val_mse: 548.2566\n",
            "Epoch 81/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 510.0593 - mse: 511.3953 - val_loss: 547.2287 - val_mse: 547.5073\n",
            "Epoch 82/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 516.5113 - mse: 510.6804 - val_loss: 546.4774 - val_mse: 546.7559\n",
            "Epoch 83/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 499.0398 - mse: 509.9560 - val_loss: 545.7337 - val_mse: 546.0121\n",
            "Epoch 84/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 523.2632 - mse: 509.2445 - val_loss: 544.9859 - val_mse: 545.2642\n",
            "Epoch 85/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 507.2748 - mse: 508.5294 - val_loss: 544.2398 - val_mse: 544.5180\n",
            "Epoch 86/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 505.4758 - mse: 507.8134 - val_loss: 543.4957 - val_mse: 543.7738\n",
            "Epoch 87/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 521.7230 - mse: 507.1062 - val_loss: 542.7467 - val_mse: 543.0248\n",
            "Epoch 88/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 512.9738 - mse: 506.3939 - val_loss: 541.9998 - val_mse: 542.2778\n",
            "Epoch 89/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 499.6260 - mse: 505.6740 - val_loss: 541.2613 - val_mse: 541.5392\n",
            "Epoch 90/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 500.6846 - mse: 504.9648 - val_loss: 540.5215 - val_mse: 540.7993\n",
            "Epoch 91/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 490.8294 - mse: 504.2575 - val_loss: 539.7796 - val_mse: 540.0573\n",
            "Epoch 92/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 508.7386 - mse: 503.5470 - val_loss: 539.0386 - val_mse: 539.3162\n",
            "Epoch 93/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 507.4024 - mse: 502.8359 - val_loss: 538.3010 - val_mse: 538.5784\n",
            "Epoch 94/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 506.1999 - mse: 502.1341 - val_loss: 537.5592 - val_mse: 537.8365\n",
            "Epoch 95/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 511.9430 - mse: 501.4174 - val_loss: 536.8274 - val_mse: 537.1048\n",
            "Epoch 96/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 513.6670 - mse: 500.7268 - val_loss: 536.0831 - val_mse: 536.3603\n",
            "Epoch 97/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 503.6607 - mse: 500.0092 - val_loss: 535.3518 - val_mse: 535.6290\n",
            "Epoch 98/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 509.8500 - mse: 499.3138 - val_loss: 534.6138 - val_mse: 534.8909\n",
            "Epoch 99/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 505.1876 - mse: 498.6101 - val_loss: 533.8778 - val_mse: 534.1548\n",
            "Epoch 100/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 490.1635 - mse: 497.8991 - val_loss: 533.1506 - val_mse: 533.4275\n",
            "Epoch 101/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 505.5771 - mse: 497.2029 - val_loss: 532.4182 - val_mse: 532.6950\n",
            "Epoch 102/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 495.6224 - mse: 496.5034 - val_loss: 531.6858 - val_mse: 531.9625\n",
            "Epoch 103/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 496.3205 - mse: 495.8033 - val_loss: 530.9541 - val_mse: 531.2307\n",
            "Epoch 104/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 500.9556 - mse: 495.1007 - val_loss: 530.2261 - val_mse: 530.5026\n",
            "Epoch 105/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 504.0918 - mse: 494.4056 - val_loss: 529.4962 - val_mse: 529.7726\n",
            "Epoch 106/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 503.9803 - mse: 493.7093 - val_loss: 528.7668 - val_mse: 529.0431\n",
            "Epoch 107/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 490.8000 - mse: 493.0139 - val_loss: 528.0372 - val_mse: 528.3134\n",
            "Epoch 108/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 488.9196 - mse: 492.3120 - val_loss: 527.3135 - val_mse: 527.5897\n",
            "Epoch 109/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 493.4343 - mse: 491.6227 - val_loss: 526.5846 - val_mse: 526.8607\n",
            "Epoch 110/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 493.9235 - mse: 490.9243 - val_loss: 525.8602 - val_mse: 526.1361\n",
            "Epoch 111/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 486.3077 - mse: 490.2255 - val_loss: 525.1412 - val_mse: 525.4171\n",
            "Epoch 112/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 504.8419 - mse: 489.5487 - val_loss: 524.4083 - val_mse: 524.6841\n",
            "Epoch 113/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 494.2091 - mse: 488.8443 - val_loss: 523.6887 - val_mse: 523.9644\n",
            "Epoch 114/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 503.4608 - mse: 488.1628 - val_loss: 522.9617 - val_mse: 523.2374\n",
            "Epoch 115/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 490.8810 - mse: 487.4607 - val_loss: 522.2474 - val_mse: 522.5230\n",
            "Epoch 116/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 487.5739 - mse: 486.7758 - val_loss: 521.5293 - val_mse: 521.8047\n",
            "Epoch 117/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 499.3594 - mse: 486.0954 - val_loss: 520.8046 - val_mse: 521.0800\n",
            "Epoch 118/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 493.5013 - mse: 485.3955 - val_loss: 520.0924 - val_mse: 520.3676\n",
            "Epoch 119/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 484.3681 - mse: 484.7101 - val_loss: 519.3796 - val_mse: 519.6548\n",
            "Epoch 120/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 486.0396 - mse: 484.0299 - val_loss: 518.6615 - val_mse: 518.9366\n",
            "Epoch 121/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 471.0601 - mse: 483.3457 - val_loss: 517.9430 - val_mse: 518.2180\n",
            "Epoch 122/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 483.1374 - mse: 482.6557 - val_loss: 517.2289 - val_mse: 517.5038\n",
            "Epoch 123/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 472.2021 - mse: 481.9722 - val_loss: 516.5150 - val_mse: 516.7898\n",
            "Epoch 124/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 465.5124 - mse: 481.2869 - val_loss: 515.8026 - val_mse: 516.0773\n",
            "Epoch 125/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 491.2638 - mse: 480.6178 - val_loss: 515.0780 - val_mse: 515.3527\n",
            "Epoch 126/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 466.7469 - mse: 479.9207 - val_loss: 514.3691 - val_mse: 514.6437\n",
            "Epoch 127/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 484.1056 - mse: 479.2432 - val_loss: 513.6566 - val_mse: 513.9311\n",
            "Epoch 128/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 475.5560 - mse: 478.5671 - val_loss: 512.9420 - val_mse: 513.2164\n",
            "Epoch 129/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 479.8534 - mse: 477.8849 - val_loss: 512.2310 - val_mse: 512.5052\n",
            "Epoch 130/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 469.5678 - mse: 477.1977 - val_loss: 511.5288 - val_mse: 511.8029\n",
            "Epoch 131/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 473.9374 - mse: 476.5224 - val_loss: 510.8242 - val_mse: 511.0984\n",
            "Epoch 132/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 479.7778 - mse: 475.8513 - val_loss: 510.1150 - val_mse: 510.3891\n",
            "Epoch 133/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 471.0657 - mse: 475.1749 - val_loss: 509.4069 - val_mse: 509.6808\n",
            "Epoch 134/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 475.5250 - mse: 474.5007 - val_loss: 508.6982 - val_mse: 508.9720\n",
            "Epoch 135/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 486.2224 - mse: 473.8296 - val_loss: 507.9876 - val_mse: 508.2614\n",
            "Epoch 136/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 452.1504 - mse: 473.1422 - val_loss: 507.2910 - val_mse: 507.5647\n",
            "Epoch 137/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 474.3940 - mse: 472.4751 - val_loss: 506.5881 - val_mse: 506.8618\n",
            "Epoch 138/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 473.1244 - mse: 471.8076 - val_loss: 505.8821 - val_mse: 506.1556\n",
            "Epoch 139/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 480.0775 - mse: 471.1379 - val_loss: 505.1760 - val_mse: 505.4495\n",
            "Epoch 140/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 469.1952 - mse: 470.4594 - val_loss: 504.4785 - val_mse: 504.7518\n",
            "Epoch 141/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 463.9413 - mse: 469.7875 - val_loss: 503.7831 - val_mse: 504.0564\n",
            "Epoch 142/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 468.6735 - mse: 469.1280 - val_loss: 503.0793 - val_mse: 503.3525\n",
            "Epoch 143/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 462.5017 - mse: 468.4522 - val_loss: 502.3832 - val_mse: 502.6562\n",
            "Epoch 144/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 464.9210 - mse: 467.7858 - val_loss: 501.6860 - val_mse: 501.9590\n",
            "Epoch 145/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 464.6420 - mse: 467.1225 - val_loss: 500.9861 - val_mse: 501.2590\n",
            "Epoch 146/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 469.1133 - mse: 466.4574 - val_loss: 500.2864 - val_mse: 500.5592\n",
            "Epoch 147/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 458.9909 - mse: 465.7853 - val_loss: 499.5939 - val_mse: 499.8666\n",
            "Epoch 148/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 461.0742 - mse: 465.1223 - val_loss: 498.9003 - val_mse: 499.1729\n",
            "Epoch 149/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 457.0759 - mse: 464.4609 - val_loss: 498.2051 - val_mse: 498.4777\n",
            "Epoch 150/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 472.4982 - mse: 463.8025 - val_loss: 497.5065 - val_mse: 497.7790\n",
            "Epoch 151/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 453.8002 - mse: 463.1325 - val_loss: 496.8167 - val_mse: 497.0891\n",
            "Epoch 152/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 463.4876 - mse: 462.4678 - val_loss: 496.1302 - val_mse: 496.4026\n",
            "Epoch 153/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 462.4617 - mse: 461.8242 - val_loss: 495.4291 - val_mse: 495.7013\n",
            "Epoch 154/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 449.9923 - mse: 461.1521 - val_loss: 494.7399 - val_mse: 495.0121\n",
            "Epoch 155/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 457.5577 - mse: 460.4912 - val_loss: 494.0522 - val_mse: 494.3242\n",
            "Epoch 156/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 463.1760 - mse: 459.8355 - val_loss: 493.3627 - val_mse: 493.6346\n",
            "Epoch 157/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 456.2178 - mse: 459.1800 - val_loss: 492.6725 - val_mse: 492.9443\n",
            "Epoch 158/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 461.3576 - mse: 458.5284 - val_loss: 491.9790 - val_mse: 492.2507\n",
            "Epoch 159/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 460.8962 - mse: 457.8574 - val_loss: 491.3002 - val_mse: 491.5718\n",
            "Epoch 160/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 460.0845 - mse: 457.2107 - val_loss: 490.6143 - val_mse: 490.8859\n",
            "Epoch 161/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 456.0596 - mse: 456.5575 - val_loss: 489.9285 - val_mse: 490.2000\n",
            "Epoch 162/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 450.5096 - mse: 455.8926 - val_loss: 489.2538 - val_mse: 489.5252\n",
            "Epoch 163/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 436.1166 - mse: 455.2471 - val_loss: 488.5720 - val_mse: 488.8434\n",
            "Epoch 164/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 454.8896 - mse: 454.6032 - val_loss: 487.8820 - val_mse: 488.1532\n",
            "Epoch 165/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 457.7783 - mse: 453.9460 - val_loss: 487.1982 - val_mse: 487.4693\n",
            "Epoch 166/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 451.3820 - mse: 453.2934 - val_loss: 486.5172 - val_mse: 486.7882\n",
            "Epoch 167/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 455.8559 - mse: 452.6447 - val_loss: 485.8361 - val_mse: 486.1071\n",
            "Epoch 168/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 447.9026 - mse: 451.9884 - val_loss: 485.1627 - val_mse: 485.4336\n",
            "Epoch 169/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 452.8673 - mse: 451.3439 - val_loss: 484.4865 - val_mse: 484.7573\n",
            "Epoch 170/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 456.0356 - mse: 450.7029 - val_loss: 483.8048 - val_mse: 484.0755\n",
            "Epoch 171/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 456.0707 - mse: 450.0537 - val_loss: 483.1265 - val_mse: 483.3971\n",
            "Epoch 172/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 457.9858 - mse: 449.4083 - val_loss: 482.4489 - val_mse: 482.7194\n",
            "Epoch 173/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 440.0579 - mse: 448.7579 - val_loss: 481.7773 - val_mse: 482.0478\n",
            "Epoch 174/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 445.3945 - mse: 448.1261 - val_loss: 481.0948 - val_mse: 481.3651\n",
            "Epoch 175/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 439.5623 - mse: 447.4741 - val_loss: 480.4201 - val_mse: 480.6904\n",
            "Epoch 176/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 446.2480 - mse: 446.8260 - val_loss: 479.7506 - val_mse: 480.0208\n",
            "Epoch 177/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 458.9595 - mse: 446.1900 - val_loss: 479.0761 - val_mse: 479.3462\n",
            "Epoch 178/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 454.0620 - mse: 445.5484 - val_loss: 478.4035 - val_mse: 478.6735\n",
            "Epoch 179/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 457.2864 - mse: 444.9071 - val_loss: 477.7328 - val_mse: 478.0027\n",
            "Epoch 180/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 454.1069 - mse: 444.2689 - val_loss: 477.0626 - val_mse: 477.3324\n",
            "Epoch 181/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 429.8059 - mse: 443.6248 - val_loss: 476.3980 - val_mse: 476.6678\n",
            "Epoch 182/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 434.0406 - mse: 442.9887 - val_loss: 475.7309 - val_mse: 476.0006\n",
            "Epoch 183/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 444.6967 - mse: 442.3576 - val_loss: 475.0584 - val_mse: 475.3280\n",
            "Epoch 184/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 442.1432 - mse: 441.7136 - val_loss: 474.3932 - val_mse: 474.6627\n",
            "Epoch 185/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 445.4708 - mse: 441.0764 - val_loss: 473.7296 - val_mse: 473.9990\n",
            "Epoch 186/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 448.9677 - mse: 440.4461 - val_loss: 473.0621 - val_mse: 473.3314\n",
            "Epoch 187/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 439.6980 - mse: 439.8113 - val_loss: 472.3961 - val_mse: 472.6653\n",
            "Epoch 188/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 432.2552 - mse: 439.1756 - val_loss: 471.7323 - val_mse: 472.0014\n",
            "Epoch 189/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 437.7397 - mse: 438.5438 - val_loss: 471.0676 - val_mse: 471.3366\n",
            "Epoch 190/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 436.2207 - mse: 437.9131 - val_loss: 470.4022 - val_mse: 470.6712\n",
            "Epoch 191/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 433.4020 - mse: 437.2736 - val_loss: 469.7445 - val_mse: 470.0134\n",
            "Epoch 192/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 434.6932 - mse: 436.6481 - val_loss: 469.0818 - val_mse: 469.3506\n",
            "Epoch 193/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 432.9289 - mse: 436.0123 - val_loss: 468.4247 - val_mse: 468.6934\n",
            "Epoch 194/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 431.9074 - mse: 435.3855 - val_loss: 467.7656 - val_mse: 468.0342\n",
            "Epoch 195/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 427.0493 - mse: 434.7560 - val_loss: 467.1075 - val_mse: 467.3760\n",
            "Epoch 196/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 436.1504 - mse: 434.1313 - val_loss: 466.4464 - val_mse: 466.7148\n",
            "Epoch 197/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 437.7999 - mse: 433.5044 - val_loss: 465.7861 - val_mse: 466.0544\n",
            "Epoch 198/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 421.8744 - mse: 432.8745 - val_loss: 465.1299 - val_mse: 465.3982\n",
            "Epoch 199/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 425.2963 - mse: 432.2489 - val_loss: 464.4731 - val_mse: 464.7413\n",
            "Epoch 200/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 432.9827 - mse: 431.6191 - val_loss: 463.8209 - val_mse: 464.0890\n",
            "Epoch 201/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 428.0573 - mse: 430.9985 - val_loss: 463.1657 - val_mse: 463.4337\n",
            "Epoch 202/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 433.7170 - mse: 430.3699 - val_loss: 462.5149 - val_mse: 462.7829\n",
            "Epoch 203/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 442.8279 - mse: 429.7558 - val_loss: 461.8569 - val_mse: 462.1248\n",
            "Epoch 204/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 434.3858 - mse: 429.1310 - val_loss: 461.2032 - val_mse: 461.4710\n",
            "Epoch 205/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 435.4764 - mse: 428.5036 - val_loss: 460.5559 - val_mse: 460.8235\n",
            "Epoch 206/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 429.5023 - mse: 427.8849 - val_loss: 459.9077 - val_mse: 460.1753\n",
            "Epoch 207/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 420.3041 - mse: 427.2655 - val_loss: 459.2592 - val_mse: 459.5266\n",
            "Epoch 208/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 413.0392 - mse: 426.6505 - val_loss: 458.6067 - val_mse: 458.8741\n",
            "Epoch 209/600\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 414.1015 - mse: 426.0289 - val_loss: 457.9562 - val_mse: 458.2236\n",
            "Epoch 210/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 424.3511 - mse: 425.4143 - val_loss: 457.3025 - val_mse: 457.5698\n",
            "Epoch 211/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 427.8195 - mse: 424.7936 - val_loss: 456.6527 - val_mse: 456.9199\n",
            "Epoch 212/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 410.0234 - mse: 424.1677 - val_loss: 456.0122 - val_mse: 456.2793\n",
            "Epoch 213/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 420.2256 - mse: 423.5587 - val_loss: 455.3649 - val_mse: 455.6319\n",
            "Epoch 214/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 419.2715 - mse: 422.9420 - val_loss: 454.7195 - val_mse: 454.9864\n",
            "Epoch 215/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 424.1260 - mse: 422.3333 - val_loss: 454.0691 - val_mse: 454.3359\n",
            "Epoch 216/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 417.3697 - mse: 421.7091 - val_loss: 453.4291 - val_mse: 453.6958\n",
            "Epoch 217/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 427.1034 - mse: 421.0995 - val_loss: 452.7863 - val_mse: 453.0529\n",
            "Epoch 218/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 412.6524 - mse: 420.4884 - val_loss: 452.1437 - val_mse: 452.4103\n",
            "Epoch 219/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 424.8731 - mse: 419.8781 - val_loss: 451.5000 - val_mse: 451.7664\n",
            "Epoch 220/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 425.9228 - mse: 419.2660 - val_loss: 450.8586 - val_mse: 451.1250\n",
            "Epoch 221/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 422.1428 - mse: 418.6604 - val_loss: 450.2145 - val_mse: 450.4807\n",
            "Epoch 222/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 428.9630 - mse: 418.0408 - val_loss: 449.5805 - val_mse: 449.8467\n",
            "Epoch 223/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 428.4036 - mse: 417.4411 - val_loss: 448.9397 - val_mse: 449.2059\n",
            "Epoch 224/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 422.7566 - mse: 416.8263 - val_loss: 448.3069 - val_mse: 448.5730\n",
            "Epoch 225/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 434.1148 - mse: 416.2331 - val_loss: 447.6625 - val_mse: 447.9285\n",
            "Epoch 226/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 403.5489 - mse: 415.6103 - val_loss: 447.0347 - val_mse: 447.3006\n",
            "Epoch 227/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 413.1821 - mse: 415.0111 - val_loss: 446.4000 - val_mse: 446.6658\n",
            "Epoch 228/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 415.7703 - mse: 414.4080 - val_loss: 445.7638 - val_mse: 446.0295\n",
            "Epoch 229/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 426.4241 - mse: 413.8101 - val_loss: 445.1224 - val_mse: 445.3881\n",
            "Epoch 230/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 403.1835 - mse: 413.1926 - val_loss: 444.4953 - val_mse: 444.7609\n",
            "Epoch 231/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 412.5505 - mse: 412.5954 - val_loss: 443.8626 - val_mse: 444.1281\n",
            "Epoch 232/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 414.7869 - mse: 411.9926 - val_loss: 443.2309 - val_mse: 443.4962\n",
            "Epoch 233/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 406.7815 - mse: 411.3929 - val_loss: 442.5980 - val_mse: 442.8633\n",
            "Epoch 234/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 413.3585 - mse: 410.7928 - val_loss: 441.9644 - val_mse: 442.2296\n",
            "Epoch 235/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 396.3191 - mse: 410.1809 - val_loss: 441.3419 - val_mse: 441.6070\n",
            "Epoch 236/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 414.9136 - mse: 409.5983 - val_loss: 440.7034 - val_mse: 440.9684\n",
            "Epoch 237/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 414.4557 - mse: 408.9900 - val_loss: 440.0746 - val_mse: 440.3395\n",
            "Epoch 238/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 410.0032 - mse: 408.3836 - val_loss: 439.4542 - val_mse: 439.7190\n",
            "Epoch 239/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 410.8971 - mse: 407.7969 - val_loss: 438.8242 - val_mse: 439.0890\n",
            "Epoch 240/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 411.0955 - mse: 407.1974 - val_loss: 438.1975 - val_mse: 438.4622\n",
            "Epoch 241/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 400.5710 - mse: 406.6027 - val_loss: 437.5702 - val_mse: 437.8348\n",
            "Epoch 242/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 401.7865 - mse: 406.0031 - val_loss: 436.9471 - val_mse: 437.2116\n",
            "Epoch 243/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 413.1769 - mse: 405.4118 - val_loss: 436.3215 - val_mse: 436.5859\n",
            "Epoch 244/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 395.8708 - mse: 404.8121 - val_loss: 435.7021 - val_mse: 435.9665\n",
            "Epoch 245/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 401.8845 - mse: 404.2279 - val_loss: 435.0744 - val_mse: 435.3386\n",
            "Epoch 246/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 391.8833 - mse: 403.6277 - val_loss: 434.4537 - val_mse: 434.7179\n",
            "Epoch 247/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 400.8433 - mse: 403.0400 - val_loss: 433.8288 - val_mse: 434.0929\n",
            "Epoch 248/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 397.6426 - mse: 402.4421 - val_loss: 433.2106 - val_mse: 433.4746\n",
            "Epoch 249/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 396.8884 - mse: 401.8594 - val_loss: 432.5850 - val_mse: 432.8489\n",
            "Epoch 250/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 411.7556 - mse: 401.2658 - val_loss: 431.9629 - val_mse: 432.2267\n",
            "Epoch 251/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 387.1907 - mse: 400.6712 - val_loss: 431.3474 - val_mse: 431.6111\n",
            "Epoch 252/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 396.1166 - mse: 400.0869 - val_loss: 430.7277 - val_mse: 430.9914\n",
            "Epoch 253/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 386.7406 - mse: 399.4930 - val_loss: 430.1139 - val_mse: 430.3774\n",
            "Epoch 254/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 399.2462 - mse: 398.9083 - val_loss: 429.4970 - val_mse: 429.7605\n",
            "Epoch 255/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 397.8276 - mse: 398.3215 - val_loss: 428.8810 - val_mse: 429.1444\n",
            "Epoch 256/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 399.4826 - mse: 397.7417 - val_loss: 428.2596 - val_mse: 428.5229\n",
            "Epoch 257/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 375.7510 - mse: 397.1493 - val_loss: 427.6458 - val_mse: 427.9091\n",
            "Epoch 258/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 393.5980 - mse: 396.5676 - val_loss: 427.0287 - val_mse: 427.2918\n",
            "Epoch 259/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 402.3178 - mse: 395.9792 - val_loss: 426.4161 - val_mse: 426.6791\n",
            "Epoch 260/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 392.2939 - mse: 395.3935 - val_loss: 425.8069 - val_mse: 426.0699\n",
            "Epoch 261/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 404.3138 - mse: 394.8193 - val_loss: 425.1906 - val_mse: 425.4535\n",
            "Epoch 262/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 393.8916 - mse: 394.2320 - val_loss: 424.5806 - val_mse: 424.8435\n",
            "Epoch 263/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 382.2330 - mse: 393.6535 - val_loss: 423.9687 - val_mse: 424.2314\n",
            "Epoch 264/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 404.9117 - mse: 393.0736 - val_loss: 423.3561 - val_mse: 423.6187\n",
            "Epoch 265/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 390.5335 - mse: 392.4919 - val_loss: 422.7470 - val_mse: 423.0096\n",
            "Epoch 266/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 390.4059 - mse: 391.9132 - val_loss: 422.1377 - val_mse: 422.4002\n",
            "Epoch 267/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 396.4532 - mse: 391.3340 - val_loss: 421.5299 - val_mse: 421.7923\n",
            "Epoch 268/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 395.9614 - mse: 390.7563 - val_loss: 420.9232 - val_mse: 421.1855\n",
            "Epoch 269/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 372.0666 - mse: 390.1724 - val_loss: 420.3240 - val_mse: 420.5862\n",
            "Epoch 270/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 393.6720 - mse: 389.6099 - val_loss: 419.7113 - val_mse: 419.9734\n",
            "Epoch 271/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 379.6210 - mse: 389.0278 - val_loss: 419.1053 - val_mse: 419.3673\n",
            "Epoch 272/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 393.6540 - mse: 388.4590 - val_loss: 418.4940 - val_mse: 418.7559\n",
            "Epoch 273/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 395.1340 - mse: 387.8793 - val_loss: 417.8892 - val_mse: 418.1511\n",
            "Epoch 274/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 383.0048 - mse: 387.3025 - val_loss: 417.2888 - val_mse: 417.5506\n",
            "Epoch 275/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 384.0487 - mse: 386.7308 - val_loss: 416.6878 - val_mse: 416.9496\n",
            "Epoch 276/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 378.3327 - mse: 386.1537 - val_loss: 416.0918 - val_mse: 416.3534\n",
            "Epoch 277/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 383.3414 - mse: 385.5848 - val_loss: 415.4931 - val_mse: 415.7547\n",
            "Epoch 278/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 367.5126 - mse: 385.0178 - val_loss: 414.8910 - val_mse: 415.1525\n",
            "Epoch 279/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 389.6895 - mse: 384.4478 - val_loss: 414.2886 - val_mse: 414.5499\n",
            "Epoch 280/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 376.2594 - mse: 383.8733 - val_loss: 413.6913 - val_mse: 413.9526\n",
            "Epoch 281/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 394.2059 - mse: 383.3122 - val_loss: 413.0873 - val_mse: 413.3485\n",
            "Epoch 282/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 370.5700 - mse: 382.7336 - val_loss: 412.4942 - val_mse: 412.7553\n",
            "Epoch 283/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 375.8494 - mse: 382.1785 - val_loss: 411.8894 - val_mse: 412.1504\n",
            "Epoch 284/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 382.4453 - mse: 381.6030 - val_loss: 411.2926 - val_mse: 411.5535\n",
            "Epoch 285/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 393.2750 - mse: 381.0376 - val_loss: 410.6959 - val_mse: 410.9567\n",
            "Epoch 286/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 387.4164 - mse: 380.4647 - val_loss: 410.1078 - val_mse: 410.3686\n",
            "Epoch 287/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 382.2276 - mse: 379.9142 - val_loss: 409.5076 - val_mse: 409.7682\n",
            "Epoch 288/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 392.0690 - mse: 379.3425 - val_loss: 408.9152 - val_mse: 409.1758\n",
            "Epoch 289/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 376.3783 - mse: 378.7783 - val_loss: 408.3248 - val_mse: 408.5853\n",
            "Epoch 290/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 385.4716 - mse: 378.2196 - val_loss: 407.7312 - val_mse: 407.9916\n",
            "Epoch 291/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 389.8271 - mse: 377.6612 - val_loss: 407.1359 - val_mse: 407.3962\n",
            "Epoch 292/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 378.7856 - mse: 377.0933 - val_loss: 406.5480 - val_mse: 406.8083\n",
            "Epoch 293/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 377.6184 - mse: 376.5288 - val_loss: 405.9641 - val_mse: 406.2242\n",
            "Epoch 294/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 379.4973 - mse: 375.9753 - val_loss: 405.3740 - val_mse: 405.6341\n",
            "Epoch 295/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 375.2943 - mse: 375.4139 - val_loss: 404.7864 - val_mse: 405.0464\n",
            "Epoch 296/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 383.2959 - mse: 374.8563 - val_loss: 404.1982 - val_mse: 404.4581\n",
            "Epoch 297/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 371.1491 - mse: 374.3022 - val_loss: 403.6073 - val_mse: 403.8672\n",
            "Epoch 298/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 364.9709 - mse: 373.7373 - val_loss: 403.0233 - val_mse: 403.2831\n",
            "Epoch 299/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 381.7394 - mse: 373.1865 - val_loss: 402.4337 - val_mse: 402.6933\n",
            "Epoch 300/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 377.6264 - mse: 372.6305 - val_loss: 401.8450 - val_mse: 402.1046\n",
            "Epoch 301/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 367.2919 - mse: 372.0629 - val_loss: 401.2682 - val_mse: 401.5276\n",
            "Epoch 302/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 366.7232 - mse: 371.5159 - val_loss: 400.6837 - val_mse: 400.9431\n",
            "Epoch 303/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 381.7431 - mse: 370.9675 - val_loss: 400.0940 - val_mse: 400.3534\n",
            "Epoch 304/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 369.0678 - mse: 370.4114 - val_loss: 399.5085 - val_mse: 399.7677\n",
            "Epoch 305/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 363.1304 - mse: 369.8510 - val_loss: 398.9302 - val_mse: 399.1894\n",
            "Epoch 306/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 357.1749 - mse: 369.2975 - val_loss: 398.3530 - val_mse: 398.6121\n",
            "Epoch 307/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 377.5395 - mse: 368.7591 - val_loss: 397.7632 - val_mse: 398.0222\n",
            "Epoch 308/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 376.1513 - mse: 368.1970 - val_loss: 397.1847 - val_mse: 397.4436\n",
            "Epoch 309/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 368.4948 - mse: 367.6489 - val_loss: 396.6057 - val_mse: 396.8646\n",
            "Epoch 310/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 359.8667 - mse: 367.0937 - val_loss: 396.0321 - val_mse: 396.2908\n",
            "Epoch 311/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 365.5676 - mse: 366.5538 - val_loss: 395.4498 - val_mse: 395.7085\n",
            "Epoch 312/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 382.3094 - mse: 366.0124 - val_loss: 394.8623 - val_mse: 395.1208\n",
            "Epoch 313/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 355.8312 - mse: 365.4470 - val_loss: 394.2929 - val_mse: 394.5514\n",
            "Epoch 314/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 363.5887 - mse: 364.9054 - val_loss: 393.7177 - val_mse: 393.9761\n",
            "Epoch 315/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 357.4818 - mse: 364.3594 - val_loss: 393.1421 - val_mse: 393.4005\n",
            "Epoch 316/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 356.9393 - mse: 363.8101 - val_loss: 392.5695 - val_mse: 392.8278\n",
            "Epoch 317/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 365.1558 - mse: 363.2668 - val_loss: 391.9953 - val_mse: 392.2534\n",
            "Epoch 318/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 353.7061 - mse: 362.7190 - val_loss: 391.4246 - val_mse: 391.6826\n",
            "Epoch 319/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 368.3555 - mse: 362.1843 - val_loss: 390.8450 - val_mse: 391.1030\n",
            "Epoch 320/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 372.0313 - mse: 361.6370 - val_loss: 390.2703 - val_mse: 390.5282\n",
            "Epoch 321/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 358.3570 - mse: 361.0917 - val_loss: 389.6993 - val_mse: 389.9571\n",
            "Epoch 322/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 361.4309 - mse: 360.5544 - val_loss: 389.1243 - val_mse: 389.3821\n",
            "Epoch 323/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 367.7873 - mse: 360.0098 - val_loss: 388.5529 - val_mse: 388.8105\n",
            "Epoch 324/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 347.9634 - mse: 359.4649 - val_loss: 387.9864 - val_mse: 388.2440\n",
            "Epoch 325/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 357.1525 - mse: 358.9245 - val_loss: 387.4196 - val_mse: 387.6771\n",
            "Epoch 326/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 350.6081 - mse: 358.3874 - val_loss: 386.8509 - val_mse: 387.1083\n",
            "Epoch 327/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 344.6904 - mse: 357.8468 - val_loss: 386.2840 - val_mse: 386.5414\n",
            "Epoch 328/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 363.3783 - mse: 357.3178 - val_loss: 385.7087 - val_mse: 385.9659\n",
            "Epoch 329/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 347.3331 - mse: 356.7699 - val_loss: 385.1437 - val_mse: 385.4008\n",
            "Epoch 330/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 351.1766 - mse: 356.2338 - val_loss: 384.5778 - val_mse: 384.8348\n",
            "Epoch 331/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 363.5709 - mse: 355.7033 - val_loss: 384.0069 - val_mse: 384.2639\n",
            "Epoch 332/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 364.5976 - mse: 355.1656 - val_loss: 383.4398 - val_mse: 383.6967\n",
            "Epoch 333/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 344.2440 - mse: 354.6261 - val_loss: 382.8785 - val_mse: 383.1353\n",
            "Epoch 334/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 357.6646 - mse: 354.0949 - val_loss: 382.3144 - val_mse: 382.5711\n",
            "Epoch 335/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 356.8133 - mse: 353.5609 - val_loss: 381.7518 - val_mse: 382.0085\n",
            "Epoch 336/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 356.0959 - mse: 353.0279 - val_loss: 381.1904 - val_mse: 381.4470\n",
            "Epoch 337/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 348.1282 - mse: 352.4906 - val_loss: 380.6343 - val_mse: 380.8908\n",
            "Epoch 338/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 351.8828 - mse: 351.9640 - val_loss: 380.0738 - val_mse: 380.3302\n",
            "Epoch 339/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 360.6458 - mse: 351.4410 - val_loss: 379.5067 - val_mse: 379.7630\n",
            "Epoch 340/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 349.7756 - mse: 350.8968 - val_loss: 378.9543 - val_mse: 379.2106\n",
            "Epoch 341/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 338.1368 - mse: 350.3705 - val_loss: 378.3988 - val_mse: 378.6549\n",
            "Epoch 342/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 350.8915 - mse: 349.8492 - val_loss: 377.8354 - val_mse: 378.0915\n",
            "Epoch 343/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 356.8128 - mse: 349.3178 - val_loss: 377.2755 - val_mse: 377.5315\n",
            "Epoch 344/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 347.8729 - mse: 348.7855 - val_loss: 376.7209 - val_mse: 376.9768\n",
            "Epoch 345/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 341.1804 - mse: 348.2570 - val_loss: 376.1677 - val_mse: 376.4235\n",
            "Epoch 346/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 344.5945 - mse: 347.7327 - val_loss: 375.6118 - val_mse: 375.8676\n",
            "Epoch 347/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 347.8840 - mse: 347.2101 - val_loss: 375.0534 - val_mse: 375.3091\n",
            "Epoch 348/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 343.8823 - mse: 346.6840 - val_loss: 374.4966 - val_mse: 374.7522\n",
            "Epoch 349/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 335.8163 - mse: 346.1533 - val_loss: 373.9457 - val_mse: 374.2012\n",
            "Epoch 350/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 339.1927 - mse: 345.6347 - val_loss: 373.3894 - val_mse: 373.6447\n",
            "Epoch 351/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 349.0196 - mse: 345.1118 - val_loss: 372.8328 - val_mse: 373.0881\n",
            "Epoch 352/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 348.0323 - mse: 344.5842 - val_loss: 372.2816 - val_mse: 372.5368\n",
            "Epoch 353/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 349.6719 - mse: 344.0659 - val_loss: 371.7281 - val_mse: 371.9833\n",
            "Epoch 354/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 350.3182 - mse: 343.5404 - val_loss: 371.1795 - val_mse: 371.4345\n",
            "Epoch 355/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 339.5719 - mse: 343.0228 - val_loss: 370.6287 - val_mse: 370.8836\n",
            "Epoch 356/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 328.7455 - mse: 342.4959 - val_loss: 370.0842 - val_mse: 370.3391\n",
            "Epoch 357/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 337.7719 - mse: 341.9818 - val_loss: 369.5340 - val_mse: 369.7888\n",
            "Epoch 358/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 339.5223 - mse: 341.4557 - val_loss: 368.9908 - val_mse: 369.2455\n",
            "Epoch 359/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 342.9928 - mse: 340.9455 - val_loss: 368.4402 - val_mse: 368.6948\n",
            "Epoch 360/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 345.1902 - mse: 340.4225 - val_loss: 367.8949 - val_mse: 368.1495\n",
            "Epoch 361/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 344.8987 - mse: 339.9117 - val_loss: 367.3447 - val_mse: 367.5992\n",
            "Epoch 362/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 344.5765 - mse: 339.3906 - val_loss: 366.7999 - val_mse: 367.0543\n",
            "Epoch 363/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 340.2370 - mse: 338.8765 - val_loss: 366.2541 - val_mse: 366.5084\n",
            "Epoch 364/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 333.2368 - mse: 338.3574 - val_loss: 365.7124 - val_mse: 365.9666\n",
            "Epoch 365/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 338.9839 - mse: 337.8434 - val_loss: 365.1705 - val_mse: 365.4247\n",
            "Epoch 366/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 332.9167 - mse: 337.3344 - val_loss: 364.6243 - val_mse: 364.8783\n",
            "Epoch 367/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 332.1715 - mse: 336.8202 - val_loss: 364.0795 - val_mse: 364.3335\n",
            "Epoch 368/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 340.3058 - mse: 336.3019 - val_loss: 363.5397 - val_mse: 363.7936\n",
            "Epoch 369/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 320.5529 - mse: 335.7903 - val_loss: 363.0001 - val_mse: 363.2539\n",
            "Epoch 370/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 341.4973 - mse: 335.2855 - val_loss: 362.4538 - val_mse: 362.7076\n",
            "Epoch 371/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 325.8874 - mse: 334.7666 - val_loss: 361.9160 - val_mse: 362.1696\n",
            "Epoch 372/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 338.4651 - mse: 334.2612 - val_loss: 361.3731 - val_mse: 361.6266\n",
            "Epoch 373/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 337.9389 - mse: 333.7504 - val_loss: 360.8319 - val_mse: 361.0854\n",
            "Epoch 374/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 322.7217 - mse: 333.2317 - val_loss: 360.3003 - val_mse: 360.5536\n",
            "Epoch 375/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 331.7014 - mse: 332.7357 - val_loss: 359.7566 - val_mse: 360.0099\n",
            "Epoch 376/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 331.8801 - mse: 332.2181 - val_loss: 359.2228 - val_mse: 359.4760\n",
            "Epoch 377/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 327.4126 - mse: 331.7089 - val_loss: 358.6910 - val_mse: 358.9442\n",
            "Epoch 378/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 332.1395 - mse: 331.2157 - val_loss: 358.1470 - val_mse: 358.4001\n",
            "Epoch 379/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 319.6048 - mse: 330.6956 - val_loss: 357.6166 - val_mse: 357.8696\n",
            "Epoch 380/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 336.7477 - mse: 330.2024 - val_loss: 357.0752 - val_mse: 357.3280\n",
            "Epoch 381/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 330.3146 - mse: 329.6877 - val_loss: 356.5437 - val_mse: 356.7965\n",
            "Epoch 382/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 328.5183 - mse: 329.1885 - val_loss: 356.0085 - val_mse: 356.2613\n",
            "Epoch 383/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 317.3419 - mse: 328.6812 - val_loss: 355.4774 - val_mse: 355.7300\n",
            "Epoch 384/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 343.6572 - mse: 328.1871 - val_loss: 354.9385 - val_mse: 355.1911\n",
            "Epoch 385/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 320.3945 - mse: 327.6687 - val_loss: 354.4164 - val_mse: 354.6688\n",
            "Epoch 386/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 321.7688 - mse: 327.1771 - val_loss: 353.8849 - val_mse: 354.1373\n",
            "Epoch 387/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 327.6222 - mse: 326.6725 - val_loss: 353.3567 - val_mse: 353.6090\n",
            "Epoch 388/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 323.7117 - mse: 326.1815 - val_loss: 352.8204 - val_mse: 353.0726\n",
            "Epoch 389/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 319.4491 - mse: 325.6715 - val_loss: 352.2939 - val_mse: 352.5461\n",
            "Epoch 390/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 322.9669 - mse: 325.1742 - val_loss: 351.7657 - val_mse: 352.0178\n",
            "Epoch 391/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 334.0548 - mse: 324.6754 - val_loss: 351.2385 - val_mse: 351.4905\n",
            "Epoch 392/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 319.5832 - mse: 324.1719 - val_loss: 350.7177 - val_mse: 350.9696\n",
            "Epoch 393/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 338.4698 - mse: 323.6867 - val_loss: 350.1860 - val_mse: 350.4379\n",
            "Epoch 394/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 323.6969 - mse: 323.1878 - val_loss: 349.6587 - val_mse: 349.9104\n",
            "Epoch 395/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 325.2497 - mse: 322.6872 - val_loss: 349.1361 - val_mse: 349.3878\n",
            "Epoch 396/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 332.1479 - mse: 322.1950 - val_loss: 348.6116 - val_mse: 348.8632\n",
            "Epoch 397/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 313.3039 - mse: 321.6932 - val_loss: 348.0952 - val_mse: 348.3467\n",
            "Epoch 398/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 312.8747 - mse: 321.2059 - val_loss: 347.5722 - val_mse: 347.8235\n",
            "Epoch 399/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 310.1976 - mse: 320.7104 - val_loss: 347.0502 - val_mse: 347.3015\n",
            "Epoch 400/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 315.2052 - mse: 320.2245 - val_loss: 346.5210 - val_mse: 346.7722\n",
            "Epoch 401/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 318.8779 - mse: 319.7188 - val_loss: 346.0038 - val_mse: 346.2550\n",
            "Epoch 402/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 323.9560 - mse: 319.2368 - val_loss: 345.4781 - val_mse: 345.7292\n",
            "Epoch 403/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 326.4759 - mse: 318.7415 - val_loss: 344.9569 - val_mse: 345.2079\n",
            "Epoch 404/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 326.6756 - mse: 318.2524 - val_loss: 344.4358 - val_mse: 344.6867\n",
            "Epoch 405/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 299.6298 - mse: 317.7521 - val_loss: 343.9253 - val_mse: 344.1761\n",
            "Epoch 406/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 325.4143 - mse: 317.2714 - val_loss: 343.4055 - val_mse: 343.6562\n",
            "Epoch 407/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 309.2660 - mse: 316.7821 - val_loss: 342.8873 - val_mse: 343.1380\n",
            "Epoch 408/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 320.2611 - mse: 316.3006 - val_loss: 342.3632 - val_mse: 342.6137\n",
            "Epoch 409/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 313.2701 - mse: 315.8023 - val_loss: 341.8497 - val_mse: 342.1002\n",
            "Epoch 410/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 316.9580 - mse: 315.3249 - val_loss: 341.3284 - val_mse: 341.5788\n",
            "Epoch 411/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 319.0347 - mse: 314.8326 - val_loss: 340.8130 - val_mse: 341.0634\n",
            "Epoch 412/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 314.2835 - mse: 314.3438 - val_loss: 340.3014 - val_mse: 340.5516\n",
            "Epoch 413/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 308.9689 - mse: 313.8569 - val_loss: 339.7920 - val_mse: 340.0421\n",
            "Epoch 414/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 321.2791 - mse: 313.3818 - val_loss: 339.2738 - val_mse: 339.5239\n",
            "Epoch 415/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 309.9116 - mse: 312.8933 - val_loss: 338.7611 - val_mse: 339.0111\n",
            "Epoch 416/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 312.2652 - mse: 312.4082 - val_loss: 338.2507 - val_mse: 338.5006\n",
            "Epoch 417/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 323.7411 - mse: 311.9329 - val_loss: 337.7343 - val_mse: 337.9841\n",
            "Epoch 418/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 306.6754 - mse: 311.4432 - val_loss: 337.2265 - val_mse: 337.4763\n",
            "Epoch 419/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 312.8804 - mse: 310.9602 - val_loss: 336.7205 - val_mse: 336.9702\n",
            "Epoch 420/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 311.9356 - mse: 310.4819 - val_loss: 336.2127 - val_mse: 336.4623\n",
            "Epoch 421/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 316.7599 - mse: 310.0035 - val_loss: 335.7037 - val_mse: 335.9532\n",
            "Epoch 422/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 309.2740 - mse: 309.5269 - val_loss: 335.1929 - val_mse: 335.4424\n",
            "Epoch 423/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 298.0800 - mse: 309.0435 - val_loss: 334.6865 - val_mse: 334.9359\n",
            "Epoch 424/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 311.5663 - mse: 308.5612 - val_loss: 334.1834 - val_mse: 334.4326\n",
            "Epoch 425/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 314.4436 - mse: 308.0939 - val_loss: 333.6709 - val_mse: 333.9201\n",
            "Epoch 426/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 305.2392 - mse: 307.6073 - val_loss: 333.1686 - val_mse: 333.4177\n",
            "Epoch 427/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 303.5451 - mse: 307.1360 - val_loss: 332.6614 - val_mse: 332.9104\n",
            "Epoch 428/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 306.6411 - mse: 306.6628 - val_loss: 332.1520 - val_mse: 332.4009\n",
            "Epoch 429/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 305.8827 - mse: 306.1745 - val_loss: 331.6555 - val_mse: 331.9044\n",
            "Epoch 430/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 303.1646 - mse: 305.7108 - val_loss: 331.1487 - val_mse: 331.3975\n",
            "Epoch 431/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 304.8748 - mse: 305.2296 - val_loss: 330.6487 - val_mse: 330.8973\n",
            "Epoch 432/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 296.2434 - mse: 304.7583 - val_loss: 330.1468 - val_mse: 330.3954\n",
            "Epoch 433/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 300.1174 - mse: 304.2876 - val_loss: 329.6424 - val_mse: 329.8910\n",
            "Epoch 434/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 308.3934 - mse: 303.8187 - val_loss: 329.1350 - val_mse: 329.3834\n",
            "Epoch 435/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 298.4733 - mse: 303.3398 - val_loss: 328.6350 - val_mse: 328.8834\n",
            "Epoch 436/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 301.9888 - mse: 302.8708 - val_loss: 328.1331 - val_mse: 328.3814\n",
            "Epoch 437/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 295.6843 - mse: 302.3936 - val_loss: 327.6371 - val_mse: 327.8853\n",
            "Epoch 438/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 304.1772 - mse: 301.9341 - val_loss: 327.1319 - val_mse: 327.3800\n",
            "Epoch 439/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 312.6572 - mse: 301.4569 - val_loss: 326.6346 - val_mse: 326.8826\n",
            "Epoch 440/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 301.0145 - mse: 300.9856 - val_loss: 326.1410 - val_mse: 326.3889\n",
            "Epoch 441/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 304.7006 - mse: 300.5233 - val_loss: 325.6422 - val_mse: 325.8900\n",
            "Epoch 442/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 306.2448 - mse: 300.0508 - val_loss: 325.1491 - val_mse: 325.3969\n",
            "Epoch 443/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 300.0396 - mse: 299.5869 - val_loss: 324.6541 - val_mse: 324.9018\n",
            "Epoch 444/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 306.1176 - mse: 299.1226 - val_loss: 324.1580 - val_mse: 324.4056\n",
            "Epoch 445/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 303.9894 - mse: 298.6567 - val_loss: 323.6631 - val_mse: 323.9106\n",
            "Epoch 446/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 289.2085 - mse: 298.1854 - val_loss: 323.1749 - val_mse: 323.4224\n",
            "Epoch 447/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 293.3955 - mse: 297.7279 - val_loss: 322.6794 - val_mse: 322.9267\n",
            "Epoch 448/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 296.6230 - mse: 297.2575 - val_loss: 322.1894 - val_mse: 322.4367\n",
            "Epoch 449/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 296.6635 - mse: 296.8010 - val_loss: 321.6931 - val_mse: 321.9403\n",
            "Epoch 450/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 302.8163 - mse: 296.3345 - val_loss: 321.2006 - val_mse: 321.4477\n",
            "Epoch 451/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 290.1905 - mse: 295.8680 - val_loss: 320.7128 - val_mse: 320.9598\n",
            "Epoch 452/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 305.2932 - mse: 295.4096 - val_loss: 320.2218 - val_mse: 320.4687\n",
            "Epoch 453/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 281.3161 - mse: 294.9452 - val_loss: 319.7348 - val_mse: 319.9817\n",
            "Epoch 454/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 295.0216 - mse: 294.4897 - val_loss: 319.2423 - val_mse: 319.4891\n",
            "Epoch 455/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 290.1476 - mse: 294.0258 - val_loss: 318.7541 - val_mse: 319.0008\n",
            "Epoch 456/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 303.6167 - mse: 293.5743 - val_loss: 318.2590 - val_mse: 318.5056\n",
            "Epoch 457/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 288.6729 - mse: 293.1053 - val_loss: 317.7751 - val_mse: 318.0217\n",
            "Epoch 458/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 299.6363 - mse: 292.6538 - val_loss: 317.2852 - val_mse: 317.5316\n",
            "Epoch 459/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 287.1470 - mse: 292.1859 - val_loss: 316.8058 - val_mse: 317.0522\n",
            "Epoch 460/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 297.0046 - mse: 291.7381 - val_loss: 316.3175 - val_mse: 316.5638\n",
            "Epoch 461/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 298.1132 - mse: 291.2788 - val_loss: 315.8328 - val_mse: 316.0790\n",
            "Epoch 462/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 282.4555 - mse: 290.8239 - val_loss: 315.3478 - val_mse: 315.5940\n",
            "Epoch 463/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 297.6620 - mse: 290.3705 - val_loss: 314.8610 - val_mse: 315.1071\n",
            "Epoch 464/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 283.4533 - mse: 289.9114 - val_loss: 314.3793 - val_mse: 314.6252\n",
            "Epoch 465/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 276.2119 - mse: 289.4580 - val_loss: 313.8967 - val_mse: 314.1426\n",
            "Epoch 466/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 279.9407 - mse: 289.0030 - val_loss: 313.4147 - val_mse: 313.6606\n",
            "Epoch 467/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 281.6025 - mse: 288.5484 - val_loss: 312.9339 - val_mse: 313.1797\n",
            "Epoch 468/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 295.1355 - mse: 288.1079 - val_loss: 312.4424 - val_mse: 312.6881\n",
            "Epoch 469/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 277.6414 - mse: 287.6369 - val_loss: 311.9699 - val_mse: 312.2155\n",
            "Epoch 470/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 289.1174 - mse: 287.1904 - val_loss: 311.4923 - val_mse: 311.7378\n",
            "Epoch 471/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 286.2770 - mse: 286.7498 - val_loss: 311.0065 - val_mse: 311.2519\n",
            "Epoch 472/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 299.9206 - mse: 286.2964 - val_loss: 310.5245 - val_mse: 310.7698\n",
            "Epoch 473/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 282.8289 - mse: 285.8405 - val_loss: 310.0506 - val_mse: 310.2959\n",
            "Epoch 474/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 270.9338 - mse: 285.3870 - val_loss: 309.5812 - val_mse: 309.8264\n",
            "Epoch 475/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 285.9773 - mse: 284.9463 - val_loss: 309.1037 - val_mse: 309.3488\n",
            "Epoch 476/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 295.9505 - mse: 284.5020 - val_loss: 308.6233 - val_mse: 308.8683\n",
            "Epoch 477/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 296.2355 - mse: 284.0544 - val_loss: 308.1455 - val_mse: 308.3905\n",
            "Epoch 478/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 286.9261 - mse: 283.6052 - val_loss: 307.6723 - val_mse: 307.9171\n",
            "Epoch 479/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 292.1705 - mse: 283.1608 - val_loss: 307.1986 - val_mse: 307.4433\n",
            "Epoch 480/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 291.3246 - mse: 282.7236 - val_loss: 306.7191 - val_mse: 306.9637\n",
            "Epoch 481/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 279.4956 - mse: 282.2653 - val_loss: 306.2536 - val_mse: 306.4982\n",
            "Epoch 482/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 278.7700 - mse: 281.8300 - val_loss: 305.7800 - val_mse: 306.0245\n",
            "Epoch 483/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 280.4672 - mse: 281.3809 - val_loss: 305.3117 - val_mse: 305.5561\n",
            "Epoch 484/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 280.9628 - mse: 280.9433 - val_loss: 304.8390 - val_mse: 305.0833\n",
            "Epoch 485/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 285.7217 - mse: 280.4991 - val_loss: 304.3686 - val_mse: 304.6128\n",
            "Epoch 486/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 282.5668 - mse: 280.0620 - val_loss: 303.8949 - val_mse: 304.1391\n",
            "Epoch 487/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 277.3787 - mse: 279.6152 - val_loss: 303.4273 - val_mse: 303.6714\n",
            "Epoch 488/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 278.3714 - mse: 279.1784 - val_loss: 302.9565 - val_mse: 303.2005\n",
            "Epoch 489/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 280.9681 - mse: 278.7373 - val_loss: 302.4871 - val_mse: 302.7311\n",
            "Epoch 490/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 276.2042 - mse: 278.2963 - val_loss: 302.0201 - val_mse: 302.2639\n",
            "Epoch 491/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 276.0402 - mse: 277.8582 - val_loss: 301.5524 - val_mse: 301.7962\n",
            "Epoch 492/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 270.6844 - mse: 277.4199 - val_loss: 301.0851 - val_mse: 301.3288\n",
            "Epoch 493/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 278.6894 - mse: 276.9795 - val_loss: 300.6204 - val_mse: 300.8640\n",
            "Epoch 494/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 288.1043 - mse: 276.5464 - val_loss: 300.1529 - val_mse: 300.3964\n",
            "Epoch 495/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 279.2319 - mse: 276.1091 - val_loss: 299.6878 - val_mse: 299.9312\n",
            "Epoch 496/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 281.9514 - mse: 275.6701 - val_loss: 299.2262 - val_mse: 299.4695\n",
            "Epoch 497/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 282.1729 - mse: 275.2399 - val_loss: 298.7607 - val_mse: 299.0040\n",
            "Epoch 498/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 286.1394 - mse: 274.8018 - val_loss: 298.2998 - val_mse: 298.5430\n",
            "Epoch 499/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 269.6052 - mse: 274.3674 - val_loss: 297.8403 - val_mse: 298.0834\n",
            "Epoch 500/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 284.2774 - mse: 273.9379 - val_loss: 297.3771 - val_mse: 297.6201\n",
            "Epoch 501/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 271.8708 - mse: 273.5013 - val_loss: 296.9185 - val_mse: 297.1615\n",
            "Epoch 502/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 281.2903 - mse: 273.0692 - val_loss: 296.4607 - val_mse: 296.7036\n",
            "Epoch 503/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 263.4025 - mse: 272.6393 - val_loss: 296.0023 - val_mse: 296.2451\n",
            "Epoch 504/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 260.2461 - mse: 272.2093 - val_loss: 295.5420 - val_mse: 295.7848\n",
            "Epoch 505/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 271.1957 - mse: 271.7854 - val_loss: 295.0753 - val_mse: 295.3179\n",
            "Epoch 506/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 282.5236 - mse: 271.3484 - val_loss: 294.6158 - val_mse: 294.8583\n",
            "Epoch 507/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 266.1437 - mse: 270.9142 - val_loss: 294.1624 - val_mse: 294.4049\n",
            "Epoch 508/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 265.3777 - mse: 270.4913 - val_loss: 293.7032 - val_mse: 293.9456\n",
            "Epoch 509/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 267.8845 - mse: 270.0607 - val_loss: 293.2465 - val_mse: 293.4889\n",
            "Epoch 510/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 274.9230 - mse: 269.6352 - val_loss: 292.7877 - val_mse: 293.0299\n",
            "Epoch 511/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 272.6812 - mse: 269.2050 - val_loss: 292.3328 - val_mse: 292.5749\n",
            "Epoch 512/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 261.5406 - mse: 268.7749 - val_loss: 291.8816 - val_mse: 292.1237\n",
            "Epoch 513/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 263.1499 - mse: 268.3493 - val_loss: 291.4297 - val_mse: 291.6717\n",
            "Epoch 514/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 264.4021 - mse: 267.9278 - val_loss: 290.9736 - val_mse: 291.2155\n",
            "Epoch 515/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 264.7878 - mse: 267.5053 - val_loss: 290.5154 - val_mse: 290.7573\n",
            "Epoch 516/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 266.4851 - mse: 267.0798 - val_loss: 290.0589 - val_mse: 290.3006\n",
            "Epoch 517/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 272.1819 - mse: 266.6454 - val_loss: 289.6126 - val_mse: 289.8542\n",
            "Epoch 518/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 267.9706 - mse: 266.2319 - val_loss: 289.1581 - val_mse: 289.3997\n",
            "Epoch 519/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 273.2444 - mse: 265.8015 - val_loss: 288.7119 - val_mse: 288.9533\n",
            "Epoch 520/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 264.8943 - mse: 265.3895 - val_loss: 288.2571 - val_mse: 288.4985\n",
            "Epoch 521/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 255.1413 - mse: 264.9535 - val_loss: 287.8164 - val_mse: 288.0577\n",
            "Epoch 522/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 257.9540 - mse: 264.5443 - val_loss: 287.3635 - val_mse: 287.6047\n",
            "Epoch 523/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 267.4270 - mse: 264.1196 - val_loss: 286.9144 - val_mse: 287.1556\n",
            "Epoch 524/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 258.3691 - mse: 263.7002 - val_loss: 286.4656 - val_mse: 286.7067\n",
            "Epoch 525/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 252.4908 - mse: 263.2713 - val_loss: 286.0261 - val_mse: 286.2671\n",
            "Epoch 526/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 273.5071 - mse: 262.8758 - val_loss: 285.5645 - val_mse: 285.8054\n",
            "Epoch 527/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 263.7697 - mse: 262.4420 - val_loss: 285.1187 - val_mse: 285.3596\n",
            "Epoch 528/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 267.2351 - mse: 262.0260 - val_loss: 284.6721 - val_mse: 284.9128\n",
            "Epoch 529/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 263.6786 - mse: 261.6123 - val_loss: 284.2236 - val_mse: 284.4643\n",
            "Epoch 530/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 264.3531 - mse: 261.1901 - val_loss: 283.7816 - val_mse: 284.0222\n",
            "Epoch 531/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 271.7579 - mse: 260.7792 - val_loss: 283.3362 - val_mse: 283.5768\n",
            "Epoch 532/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 256.6380 - mse: 260.3580 - val_loss: 282.8980 - val_mse: 283.1385\n",
            "Epoch 533/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 256.7428 - mse: 259.9463 - val_loss: 282.4573 - val_mse: 282.6976\n",
            "Epoch 534/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 259.0038 - mse: 259.5347 - val_loss: 282.0146 - val_mse: 282.2549\n",
            "Epoch 535/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 264.3011 - mse: 259.1230 - val_loss: 281.5710 - val_mse: 281.8112\n",
            "Epoch 536/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 250.1705 - mse: 258.7109 - val_loss: 281.1275 - val_mse: 281.3676\n",
            "Epoch 537/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 262.5557 - mse: 258.2956 - val_loss: 280.6870 - val_mse: 280.9271\n",
            "Epoch 538/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 263.0809 - mse: 257.8832 - val_loss: 280.2478 - val_mse: 280.4878\n",
            "Epoch 539/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 261.3401 - mse: 257.4708 - val_loss: 279.8109 - val_mse: 280.0508\n",
            "Epoch 540/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 254.6962 - mse: 257.0615 - val_loss: 279.3733 - val_mse: 279.6131\n",
            "Epoch 541/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 259.1862 - mse: 256.6522 - val_loss: 278.9352 - val_mse: 279.1749\n",
            "Epoch 542/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 259.3730 - mse: 256.2445 - val_loss: 278.4962 - val_mse: 278.7358\n",
            "Epoch 543/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 253.8233 - mse: 255.8366 - val_loss: 278.0569 - val_mse: 278.2965\n",
            "Epoch 544/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 244.8039 - mse: 255.4247 - val_loss: 277.6217 - val_mse: 277.8611\n",
            "Epoch 545/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 256.8992 - mse: 255.0213 - val_loss: 277.1823 - val_mse: 277.4217\n",
            "Epoch 546/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 250.7125 - mse: 254.6062 - val_loss: 276.7508 - val_mse: 276.9901\n",
            "Epoch 547/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 252.6916 - mse: 254.1961 - val_loss: 276.3227 - val_mse: 276.5619\n",
            "Epoch 548/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 251.0939 - mse: 253.8001 - val_loss: 275.8850 - val_mse: 276.1242\n",
            "Epoch 549/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 265.5442 - mse: 253.3952 - val_loss: 275.4471 - val_mse: 275.6862\n",
            "Epoch 550/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 249.9449 - mse: 252.9825 - val_loss: 275.0181 - val_mse: 275.2571\n",
            "Epoch 551/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 244.5870 - mse: 252.5823 - val_loss: 274.5851 - val_mse: 274.8240\n",
            "Epoch 552/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 246.3948 - mse: 252.1790 - val_loss: 274.1513 - val_mse: 274.3901\n",
            "Epoch 553/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 251.4749 - mse: 251.7772 - val_loss: 273.7160 - val_mse: 273.9547\n",
            "Epoch 554/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 247.3036 - mse: 251.3700 - val_loss: 273.2851 - val_mse: 273.5238\n",
            "Epoch 555/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 252.4711 - mse: 250.9683 - val_loss: 272.8543 - val_mse: 273.0929\n",
            "Epoch 556/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 254.5376 - mse: 250.5657 - val_loss: 272.4250 - val_mse: 272.6636\n",
            "Epoch 557/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 245.6154 - mse: 250.1631 - val_loss: 271.9978 - val_mse: 272.2362\n",
            "Epoch 558/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 261.7164 - mse: 249.7729 - val_loss: 271.5612 - val_mse: 271.7996\n",
            "Epoch 559/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 256.6568 - mse: 249.3628 - val_loss: 271.1360 - val_mse: 271.3743\n",
            "Epoch 560/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 254.0386 - mse: 248.9621 - val_loss: 270.7133 - val_mse: 270.9514\n",
            "Epoch 561/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 242.8181 - mse: 248.5650 - val_loss: 270.2897 - val_mse: 270.5278\n",
            "Epoch 562/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 252.6359 - mse: 248.1721 - val_loss: 269.8611 - val_mse: 270.0991\n",
            "Epoch 563/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 246.7229 - mse: 247.7720 - val_loss: 269.4357 - val_mse: 269.6737\n",
            "Epoch 564/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 257.4322 - mse: 247.3708 - val_loss: 269.0147 - val_mse: 269.2526\n",
            "Epoch 565/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 245.0077 - mse: 246.9835 - val_loss: 268.5861 - val_mse: 268.8239\n",
            "Epoch 566/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 255.8608 - mse: 246.5822 - val_loss: 268.1635 - val_mse: 268.4012\n",
            "Epoch 567/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 249.6573 - mse: 246.1933 - val_loss: 267.7365 - val_mse: 267.9741\n",
            "Epoch 568/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 238.6329 - mse: 245.7887 - val_loss: 267.3194 - val_mse: 267.5569\n",
            "Epoch 569/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 252.2708 - mse: 245.4052 - val_loss: 266.8916 - val_mse: 267.1291\n",
            "Epoch 570/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 248.1072 - mse: 245.0075 - val_loss: 266.4689 - val_mse: 266.7062\n",
            "Epoch 571/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 239.4142 - mse: 244.6115 - val_loss: 266.0494 - val_mse: 266.2867\n",
            "Epoch 572/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 243.5888 - mse: 244.2195 - val_loss: 265.6299 - val_mse: 265.8671\n",
            "Epoch 573/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 245.8383 - mse: 243.8282 - val_loss: 265.2103 - val_mse: 265.4475\n",
            "Epoch 574/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 247.4250 - mse: 243.4377 - val_loss: 264.7905 - val_mse: 265.0276\n",
            "Epoch 575/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 242.0117 - mse: 243.0518 - val_loss: 264.3673 - val_mse: 264.6043\n",
            "Epoch 576/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 234.9860 - mse: 242.6525 - val_loss: 263.9524 - val_mse: 264.1893\n",
            "Epoch 577/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 247.0743 - mse: 242.2661 - val_loss: 263.5341 - val_mse: 263.7709\n",
            "Epoch 578/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 236.2411 - mse: 241.8711 - val_loss: 263.1221 - val_mse: 263.3589\n",
            "Epoch 579/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 250.2068 - mse: 241.4931 - val_loss: 262.6999 - val_mse: 262.9366\n",
            "Epoch 580/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 234.9299 - mse: 241.0981 - val_loss: 262.2851 - val_mse: 262.5217\n",
            "Epoch 581/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 237.2231 - mse: 240.7145 - val_loss: 261.8668 - val_mse: 262.1033\n",
            "Epoch 582/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 242.0335 - mse: 240.3279 - val_loss: 261.4486 - val_mse: 261.6850\n",
            "Epoch 583/600\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 242.5291 - mse: 239.9349 - val_loss: 261.0373 - val_mse: 261.2737\n",
            "Epoch 584/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 233.6225 - mse: 239.5465 - val_loss: 260.6289 - val_mse: 260.8651\n",
            "Epoch 585/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 239.1514 - mse: 239.1684 - val_loss: 260.2131 - val_mse: 260.4493\n",
            "Epoch 586/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 235.9118 - mse: 238.7809 - val_loss: 259.8005 - val_mse: 260.0366\n",
            "Epoch 587/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 238.8452 - mse: 238.3975 - val_loss: 259.3875 - val_mse: 259.6235\n",
            "Epoch 588/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 235.9537 - mse: 238.0100 - val_loss: 258.9782 - val_mse: 259.2141\n",
            "Epoch 589/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 238.1842 - mse: 237.6284 - val_loss: 258.5676 - val_mse: 258.8035\n",
            "Epoch 590/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 229.1318 - mse: 237.2491 - val_loss: 258.1541 - val_mse: 258.3899\n",
            "Epoch 591/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 224.2135 - mse: 236.8576 - val_loss: 257.7492 - val_mse: 257.9849\n",
            "Epoch 592/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 236.7808 - mse: 236.4821 - val_loss: 257.3376 - val_mse: 257.5732\n",
            "Epoch 593/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 232.7240 - mse: 236.1003 - val_loss: 256.9273 - val_mse: 257.1629\n",
            "Epoch 594/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 237.5672 - mse: 235.7224 - val_loss: 256.5151 - val_mse: 256.7505\n",
            "Epoch 595/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 221.1895 - mse: 235.3348 - val_loss: 256.1108 - val_mse: 256.3462\n",
            "Epoch 596/600\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 230.7339 - mse: 234.9636 - val_loss: 255.6980 - val_mse: 255.9333\n",
            "Epoch 597/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 245.3202 - mse: 234.5883 - val_loss: 255.2827 - val_mse: 255.5179\n",
            "Epoch 598/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 241.0666 - mse: 234.1955 - val_loss: 254.8830 - val_mse: 255.1181\n",
            "Epoch 599/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 233.7143 - mse: 233.8226 - val_loss: 254.4797 - val_mse: 254.7147\n",
            "Epoch 600/600\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 238.2655 - mse: 233.4477 - val_loss: 254.0750 - val_mse: 254.3099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2dc02d06a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbKSgwVGU4PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  \n",
        "  \n",
        "  tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFu2k4J_spfi",
        "colab_type": "code",
        "outputId": "b0f8c6ac-2c1a-40d3-ce91-b14fa35ca691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 46ms/step - loss: -2763.6973 - mse: 11578.6455\n",
            "Mean Squared Error - Test Data 11578.6455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp3cSNbRV476",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  \n",
        "  tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Q4c32TV5GI",
        "colab_type": "code",
        "outputId": "9931f0d1-ba95-4ad1-c2e9-0658de5a8252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 40ms/step - loss: 25881.8315 - mse: 25879.8477\n",
            "Mean Squared Error - Test Data 25879.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TWClbvZaFBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RAD=tf.feature_column.bucketized_column(RAD, boundaries=[2, 5])\n",
        "\n",
        "feature_columns.append(RAD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kcXaGMzaFFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  \n",
        "  \n",
        "  tf.keras.layers.Dense(1,activation=None)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE9GaUxYaFKS",
        "colab_type": "code",
        "outputId": "a41f6a82-3c63-4c5d-83ea-80b37ba66974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 41ms/step - loss: -1887.2136 - mse: 8716.8721\n",
            "Mean Squared Error - Test Data 8716.872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYOIjIhxaFNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}